{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 관련 기술들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 매개변수 갱신\n",
    "\n",
    "### 모험가 이야기\n",
    "### 확률적 경사 하강법(SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SGD 클래스를 활용한 신경망 매개변수 진행\n",
    "\n",
    "        network = TwoLayerNet(...)\n",
    "        optimizer = SGD()\n",
    "\n",
    "        for i in range(10000):\n",
    "            ...\n",
    "            x_batch, t_batch = get_mini_batch(...) # 미니 배치\n",
    "            grads = network.gradient(x_batch, t_batch)\n",
    "            params = network.params\n",
    "            optimizer.update(params, grads)\n",
    "            ...\n",
    "* optimizer : 최적화를 행하는 자\n",
    "SGD()가 그 역할을 한다.  \n",
    "매개변수 갱신을 optimizer가 하고, 사용자가 매개변수와 기울기 정보를 optimizer에 입력한다.\n",
    "* 최적화 담당 클래스를 분리해서 구현하면 모듈화 하기 좋다.  \n",
    "optimizer = SGD()  \n",
    "optimizer = Momentum() : SGD가 모멘텀으로 바뀐다.\n",
    "* Lasagne 딥러닝 프레임워크 : updates.py 에 최적화 기법을 구현."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD의 단점\n",
    "문제에 따라 비효율적일 때가 있다.  \n",
    "다음 함수의 최솟값 구하는 문제\n",
    "$$f(x,y) = {1 \\over 20}x^2 + y^2$$\n",
    "\n",
    "* 함수의 그래프(왼쪽), 등고선(오른쪽)\n",
    "![img](./deep_learning_images/fig_6-1.png)\n",
    "\n",
    "* 함수의 기울기\n",
    "![img](./deep_learning_images/fig_6-2.png)\n",
    "y축 방향이 크고, x축 방향은 작다.  \n",
    "\n",
    "* 주의할 점\n",
    "    * 최솟값이 되는 장소는 $(x,y) = (0,0)$이지만 그림의 기울기는 대부분 원점을 가리키지 않는다.\n",
    "    \n",
    "* SGD 적용  \n",
    "초깃값을 $(x,y) = (-7.0,2.0)$으로 지정한다.\n",
    "![img](./deep_learning_images/fig_6-3.png)\n",
    "최솟값인 원점까지 지그재그로 이동한다. 비효율적이다.  \n",
    "심하게 굽어진 움직임을 보여준다.  \n",
    "* SGD 단점\n",
    "    * 비등방성(anisotropy) 함수(기울기가 달라지는 함수)에서 탐색 경로가 비효율적이다.\n",
    "    * 근본 원인 : 기울어진 방향이 본래의 최솟값과 다른 방향을 가리킨다.\n",
    "    \n",
    "* 개선한 방법\n",
    "    * 모멘텀, AdaGrad, Adam 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모멘텀\n",
    "모멘텀(momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None #v : 물체의 속도\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
    "                params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모멘텀에 의한 최적화 갱신 경로\n",
    "![img](./deep_learning_images/fig_6-5.png)\n",
    "SGD 보다 지그재그 정도가 덜하다.  \n",
    "> x축의 힘이 아주 작지만 방향이 변하지 않아서, 한 방향으로 일정하게 가속하기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "$$\\mathbf{h} \\leftarrow \\mathbf{h} + {\\partial L \\over \\partial \\mathbf{W}} \\odot {\\partial L \\over \\partial \\mathbf{W}} \\\\\n",
    "\\mathbf{W} \\leftarrow \\mathbf{W} - \\eta{1 \\over \\sqrt{\\mathbf{h}}} {\\partial L \\over \\partial \\mathbf{W}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params, items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./deep_learning_images/fig_6-6.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "모멘텀과 AdaGrad 기법을 융합한 것이 Adam이다.\n",
    "![img](./deep_learning_images/fig_6-7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어느 갱신 방법을 사용할 것인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure(640x480)\r\n"
     ]
    }
   ],
   "source": [
    "!python ./optimizer_compare_naive.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
