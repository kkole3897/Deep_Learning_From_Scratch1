{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 학습\n",
    "* 학습 : 훈련 데이터에서 가중치 매개변수의 최적값을 자동으로 획득\n",
    "* 지표 : 손실 함수  \n",
    "* 손실 함수의 결괏값을 최소화하는 가중치 매개변수 찾기  \n",
    "* 경사법 : 함수의 기울기 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터에서 학습!\n",
    "신경망은 데이터를 보고 학습할 수 있다. 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다.  \n",
    "* 퍼셉트론 수렴 정리(perceptron convergence theorem) : 선형 분리 가능 문제는 퍼셉트론도 학습을 통해 풀 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 주도 학습\n",
    "기계학습은 데이터가 생명이다.  \n",
    "신경망과 딥러닝 : 기존 ML보다 사람의 개입을 더 배제할 수 있는 특성을 지닌다.  \n",
    "ex) 이미지에서 5라는 숫자를 인식하는 프로그램. \n",
    "![png](./deep_learning_images/fig_4-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 데이터로 해결하기  \n",
    "1. 이미지에서 특징 feature를 추출한다.  \n",
    "2. 그 특징의 패턴을 기계학습 기술로 학습한다.  \n",
    "* 특징. \n",
    "입력 데이터에서 본질적인 데이터를 정확하게 추출하도록 설계된 변환기. \n",
    "이미지 특징은 벡터로 기술, 컴퓨터 비전에서는 SIFT, SURF, HOG 특징을 활용한다.  \n",
    "> SIFT, SURF, HOG?\n",
    "\n",
    "* 이미지를 벡터로 변환할 때 사용하는 특징은 여전히 '사람'이 설계한다.  \n",
    "문제에 적합한 특징을 쓰지 않으면 좋은 결과가 나오지 않는다.  \n",
    "* 딥러닝과 신경망은 이미지에 포함된 중요한 특징까지도 '기계'가 스스로 학습한다.  \n",
    "![fig_4-2.png](./deep_learning_images/fig_4-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망은 모든 문제를 같은 맥락에서 풀 수 있다.  \n",
    "모든 문제를 주어진 데이터 그대로를 입력 데이터로 활용해 **end-to-end**로 학습할 수 있다.  \n",
    "> 딥러닝 = 종단간 기계학습 end-to-end machine learning  \n",
    "> 처음부터 끝까지라는 의미로, 데이터 입력에서 목표한 결과 출력까지 사람의 개입 없이 얻는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 데이터와 시험 데이터\n",
    "* 기계학습 문제  \n",
    "데이터 = 훈련 데이터(training data) + 시험 데이터(test data)  \n",
    "학습과 실험을 수행한다.  \n",
    "1. 훈련 데이터만 사용해서 학습하면서 최적의 매개변수 찾기. \n",
    "2. 시험 데이터로 훈련한 모델의 실력을 평가.  \n",
    "\n",
    "* 필요성   \n",
    "범용적으로 사용 가능한 모델 추구  \n",
    "범용 능력 평가를 위해 분리  \n",
    "\n",
    "* 오버 피팅(overfitting)  \n",
    "한 데이터셋에만 지나치게 최적화된 상태  \n",
    "이를 피하기는 기계학습의 중요한 과제이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실 함수\n",
    "신경망 학습에서 사용하는 지표  \n",
    "최적의 매개변수 값을 탐색하는 기준이 된다.  \n",
    "* 평균 제곱 오차(MSE), 교차 엔트로피 오차 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평균 제곱 오차\n",
    "가장 많이 쓰이는 손실 함수로, Mean Squared Error, MSE, 수식으로 다음과 같다.  \n",
    "$$E = {1 \\over 2} \\sum_k (y_k - t_k)^2$$\n",
    "$y_k$ 신경망의 출력(신경망 추정한 값)   \n",
    "$t_k$ 정답 레이블  \n",
    "$k$ 데이터의 차원의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손글씨 숫자 인식에서 10개짜리 원소 데이터\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] #소프트맥스 함수의 출력으로 각 클래스로 분류될 확률로 해석한다.\n",
    "t = [0,0,1,0,0,0,0,0,0,0] # 정답을 가리키는 위치의 원소는 1이므로, 실제 레이블은 2를 가리킨다. True이면 1로 표기한다. 따라서 여기서는 2가 정답이다.\n",
    "# 이처럼 한 원소만 1이고 나머지 0으로 표기하는 걸 원-핫 인코딩이라 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균 제곱 오차\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5*np.sum((y-t)**2) # python에서 제곱은 ** 을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#y와 t는 넘파이 배열이다.\n",
    "\n",
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "# 예1 : 2일 확률이 높은 경우\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] \n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예2 : 7일 확률이 가장 높은 경우\n",
    "y = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평균 제곱 오차를 기준으로는 첫 번째 추정 결과가 오차가 더 작으므로, 정답과 더 가까울 것으로 판단한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 교차 엔트로피 오차\n",
    "교차 엔트로피 오차(Cross entropy error, CEE)\n",
    "$$ E = -\\sum_k t_k \\log y_k$$\n",
    "$y_k$ 신경망의 출력  \n",
    "$t_k$ 정답 레이블(원-핫 인코딩)  \n",
    "이 식은 정답일 때의 추정의 자연로그 값을 계산하는 식이 된다. 정답일 때의 출력이 전체 값을 결정한다.  \n",
    "정답일 때의 출력 값이 작아질수록 오차는 커진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연로그의 그래프\n",
    "![fig_4-2.png](./deep_learning_images/fig_4-3.png)\n",
    "x가 1일 때는 y가 0이되고, x가 0에 가까워질수록 y 값이 작아진다.(음수 값)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 엔트로피 오차\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y + delta)) # delta라는 작은 값을 더하여, 로그 값에 0을 넣어 마이너스 무한대가 되는 걸 방지한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))\n",
    "# 뒤의 오차가 더 크게 나타났고, 첫 번째 추정이 정답일 가능성이 높다. MSE의 결과와 일치한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미니배치 학습\n",
    "모든 훈련 데이터를 대상으로 훈련 데이터 손실 함수 값을 구해야 한다.  \n",
    "훈련 데이터 모두에 대한 손실함수의 합을 구해본다.  \n",
    "$$E = -{1 \\over N} \\sum_n \\sum_k t_{nk} log y_{nk}$$  \n",
    "데이터가 N개이면, $t_{nk}$ : n번째 데이터의 k번째 값  \n",
    "손실함수를 단순히 N개의 데이터로 확장했다. 마지막에 N으로 나누어 정규화 실시하여, '평균 손실 함수'를 구한다.  \n",
    "* 평균 손실 함수 : 훈련 데이터 개수와 관계없이 동일한 지표를 얻는다.  \n",
    "데이터가 큰 경우, 그 일부를 추려 전체의 '근사치'로 이용한다. 신경망 학습에서도 일부 훈련 데이터로 학습한다.  \n",
    "* 미니배치 mini-batch : 훈련 데이터 일부. \n",
    "* 미니배치 학습 : 전체 훈련 데이터 중 100장(일부)를 무작위로 뽑아서 그 일부만을 사용하여 학습하는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "#미니배치 학습\n",
    "import sys, os\n",
    "sys.path.append(os.pardir) #부모 디렉토리 설정\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize= True, one_hot_label= True) #정규화하고 원핫 인코딩도 한다.\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무작위로 10장만 빼낸다. 넘파이 함수 이용한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24997, 15994, 34633, 46685, 48617, 10003, 40445,  9253, 53095,\n",
       "       41352])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 지정한 범위에서 무작위로 원하는 갯수만 뽑아낼 수 있다.\n",
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무작위 인덱스로 미니배치를 뽑는다. 손실 함수도 미니배치로 계산한다.\n",
    "# 배치용 교차 엔트로피 오차 구현하기\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정답 레이블이 원핫 인코딩이아니라 숫자레이블인경우 교차 엔트로피 오차\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size #레이블 표현에서 원-핫 인코딩과 다르게 이렇게 변형 한다. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* np.log(y[np.arange(batch_size), t])\n",
    "1. np.arange(batch_size) : 0부터 batch_size -1 까지 배열을 생성  \n",
    "2. [0,1,2,3,4] 넘파이 배열 생성.  \n",
    "3. t에는 [2,7,0,9,4]로 저장되어, y[np.arange(batch_size), t]는 각 정답 레이블에 해당하는 신경망의 출력을 추출한다.  \n",
    "4. 여기서는 [y[0,2], y[1,7], y[2,0], y[3,9]. y[4,4]] 로 출력이 될 것이다. 각각 y[넘파이 배열, 대응하는 t 정답레이블 원소]로 구성된 넘파이 배열이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실 함수의 필요성\n",
    "정확도 안쓰고 손실 함수의 값을 사용하는 이유?  \n",
    "미분의 역할 고려하면, 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없기 때문이다.  \n",
    "_신경망을 학습할 때 정확도를 지표로 삼아서는 안 된다. 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다._  \n",
    "\n",
    "정확도는 매개변수를 조정해도 개선되지 않고 일정하게 유지한다. 연속적인 변화보다는 33%나 34%처럼 불연속적인 값으로 변한다.  \n",
    "신경망에서 계단함수를 활성화함수로 사용하지 않는 이유와 같다. 매개변수의 작은 변화를 계단 함수가 무시하여 손실 함수 값에는 변화가 없게 된다.  \n",
    "계단 함수의 미분은 대부분 0이지만, 시그모이드 함수의 미분은 어느 장소에서도 0이 되지 않는다. 기울기가 0이 되지 않아서 신경망이 올바르게 작동한다.  \n",
    "![img4](./deep_learning_images/fig_4-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수치 미분\n",
    "경사법은 기울기(경사) 값을 기준으로 실시한다.  \n",
    "### 미분\n",
    "미분은 한순간의 변화량이다.  \n",
    "$${df(x) \\over dx} = \\lim_{h \\to 0} {f(x+h) - f(x) \\over h}$$  \n",
    "x의 작은 변화가 함수 f(x)를 얼마나 변화시키느냐 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나쁜 구현\n",
    "def numerical_diff(f, x): #수치 미분 numerical differentiation\n",
    "    h = 10e-50\n",
    "    return(f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개선할 점  \n",
    "1. 반올림 오차 rounding error 문제  \n",
    "작은 값이 생략되어 0.0으로 처리되므로, 최종 계산 결과 오차가 생긴다. 미세한 값 h로 $10^{-4}$를 사용하면 좋다.\n",
    "2. 함수 f의 차분  \n",
    "x+h와 x의 사이의 함수 f의 차분 계산에서 오차가 있다. 즉, x 위치의 함수의 기울기가 아니라 (x+h)와 x 사이의 기울기가 된다.  \n",
    "h를 무한히 0으로 가깝게 만들 수 없기 때문에 발생한다.\n",
    "![img5](./deep_learning_images/fig_4-5.png)  \n",
    "이 경우 중심 차분(중앙 차분)을 활용한다. (x+h)와 (x-h)일 때 함수 f의 차분을 계산한다. x를 중심으로 그 전후의 차분을 계산한다.  \n",
    "_(x+h)와 x의 차분: **전방 차분**_\n",
    "\n",
    "3. 참고 : 수치미분과 해석적 미분  \n",
    "수치 미분은 근사치를 구하고, 해석적 미분이 오차를 포함하지 않는 진정한 미분값을 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#개선한 수치미분\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치 미분의 예\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = 0.01 x^2 + 0.1x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x= 5일때와 10일때 함수의 미분\n",
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 값은 함수의 기울기에 해당한다. 해석적 해는 다음과 같다.  \n",
    "$${df(x) \\over dx} = 0.02x +0.1$$  \n",
    "따라서 x가 5, 10일때 진정한 미분은 0.2, 0.3이다. 앞의 수치 미분과 오차가 매우 적고 거의 같다.  \n",
    "![img6](./deep_learning_images/fig_4-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 편미분\n",
    "$$f(x_0, x_1) = x_0^2 + x_1^2$$\n",
    "![img7](./deep_learning_images/fig_4-8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2 # return np.sum(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변수가 2개 있고, 어느 변수에 대한 미분인가를 구분한다. 변수가 여럿인 함수에 대한 미분을 **편미분**이라 한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_0 = 3, x_1 = 4일때 x_0 편미분?\n",
    "def function_tmp1(x0):\n",
    "    return x0 * x0 + 4.0**2.0\n",
    "\n",
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_0 = 3, x_1 = 4, x_1 편미분?\n",
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1*x1\n",
    "\n",
    "numerical_diff(function_tmp2, 4.0) #해석적 미분과 거의 결과가 같다. \n",
    "# 목표 변수 하나에 초점을 맞추고 다른 변수는 값을 고정한다. 새로운 함수를 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기울기\n",
    "기울기 gradient는 모든 변수의 편미분을 벡터로 정리한 것이다. $$\\left( {\\partial f \\over \\partial x_0},{\\partial f \\over \\partial x_1} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x) #x와 형상이 같은 배열\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        #f(x+h)\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 4.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([0.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 기울기는 함수의 가장 낮은 장소(최소값)을 나타낸다. 최소값에서 멀어질수록 화살표 크기가 커진다.  \n",
    "_기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다._  \n",
    "![img8](./deep_learning_images/fig_4-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사법(경사하강법)\n",
    "경사법은 기울기로 손실 함수의 최솟값을 찾는다.\n",
    "다만, 기울기가 가리키는 곳에 함수의 최솟값이 있는지는 알 수 없다. 실제로 복잡한 경우, 없는 경우가 많다.  \n",
    "* 극솟값, 최솟값, 안장점(saddle point) : 기울기가 0인 장소이다.  \n",
    "* 고원(plateau) : 학습이 진행되지 않는 정체기   \n",
    "\n",
    "* 경사법(gradient method) : 현 위치에서 기울어진 방향으로 일정 거리만큼 이동하고, 기울기 구하고, 또 기울어진 방향으로 나아가서 함수의 값을 점차 줄이기. \n",
    "기계학습 최적화에서 많이 사용한다. 신경망 학습에서 많이 사용한다.  \n",
    "* 경사 하강법(gradient descent method), 경사 상승법(gradient ascent method) : 최솟값, 최댓값 찾는 방법이나 손실함수의 부호만 다르다. 주로 경사하강법 많이 사용한다.  \n",
    "\n",
    "$$ x_0 = x_0 - \\eta {\\partial f \\over \\partial x_0} \\\\\n",
    " x_1 = x_1 - \\eta {\\partial f \\over \\partial x_1}$$  \n",
    " $\\eta$ : 신경망 학습에서 학습률 learning rate로 갱신하는 양을 나타낸다. 한 번의 학습으로 매개변수 값을 얼마나 갱신하는 가를 정한다.  \n",
    " 보통 이 학습률을 0.01이나 0.001 등 미리 정한다. 너무 크거나 작으면 좋은 장소를 찾아갈 수 없다. 이를 변경하면서 학습 진행을 확인하고 결정한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num = 100): # f: 최적화 함수, init_x 초깃값, lr 학습률, step_num 경사법 반복횟수\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr*grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 경사법으로 위 함수의 최솟값 구하기\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x = init_x, lr = 0.1, step_num = 100) # 거의 (0,0)에 가깝다. 실제로 최솟값은 (0,0)이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 경사법에 의한 갱신 과정 : 점선은 함수의 등고선이다.\n",
    "![img9](./deep_learning_images/fig_4-10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.58983747e+13, -1.29524862e+12])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 큰 경우 : lr = 10\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x = init_x, lr = 10, step_num = 100)  #너무 큰 값으로 발산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 큰 경우 : lr = 10\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x = init_x, lr = 1e-10, step_num = 100)  # 갱신되지 못하고 끝난다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 하이퍼파라미터 hyper parameter: 학습률 같은 매개변수로, 사람이 직접 설정해야하는 매개변수이다. 여러 후보 값 중에서 시험을 통해 가장 잘 학습하는 값을 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망에서의 기울기\n",
    "가중치 매개변수에 대한 손실 함수의 기울기  \n",
    "가중치가 W, 손실 함수가 L인 신경망 : 경사 = $\\partial L \\over \\partial \\mathbf{W}$\n",
    "$$  \\mathbf{W} = \\pmatrix{w_{11} w_{12} w_{13} \\\\ w_{21} w_{22} w_{23}} \\\\\n",
    "{\\partial L \\over \\partial \\mathbf{W}} =  \\pmatrix{{\\partial L \\over w_{11}} {\\partial L \\over w_{12}} {\\partial L \\over w_{13}} \\\\ {\\partial L \\over w_{21}} {\\partial L \\over w_{22}} {\\partial L \\over w_{23}}}$$\n",
    "\n",
    "각 원소에 대한 편미분 값이다. 결국 첫번째 편미분 원소는 $w_{11}$을 조금 변경했을 때 손실 함수 L이 얼마나 변화하는가를 보여준다.  \n",
    "중요한 점은 두 행렬 모두 $2 \\times 3$ 의 형상이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기 구하는 코드\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3) #정규분포 초기화\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.5502921  -0.54133735 -0.3463818 ]\n",
      " [-0.45569383  2.05034381  0.69028705]]\n"
     ]
    }
   ],
   "source": [
    "# simpleNet 시험\n",
    "net = simpleNet()\n",
    "print(net.W) # 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.3402997   1.52050702  0.41342926]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p) #최댓값의 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4347581873029989"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0,0,1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0247405   0.4323556  -0.4570961 ]\n",
      " [ 0.03711075  0.64853339 -0.68564415]]\n"
     ]
    }
   ],
   "source": [
    "# 기울기 : 함수의 인수 W는 더미로 만든다.\n",
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 함수는 람다lambda 기법이 더 편하다.\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 알고리즘 구현하기\n",
    "신경망 학습 절차\n",
    "0. 전제\n",
    "신경망 : 적응 가능한 가중치와 편향이 있다.  \n",
    "학습 : 이 가중치와 편향을 훈련 데이터에 적응하도록 조정한다.  \n",
    "1. 미니배치\n",
    "훈련 데이터 중 일부를 무작위로 가져온다.(미니배치)  \n",
    "그 미니배치의 손실함수 줄인다.  \n",
    "2. 기울기 산출  \n",
    "미니배치의 손실 함수 값을 줄이고자 각 가중치 매개변수의 기울기 구한다.  \n",
    "기울기 : 손실 함수의 값을 가장 작게 하는 방향을 제시  \n",
    "3. 매개변수 갱신  \n",
    "가중치 매개변수를 기울기 방향으로 아주 조금 갱신  \n",
    "4. 반복. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습은 경사 하강법으로 매개변수를 갱신한다.  \n",
    "미니배치로 무작위 선정하므로, **확률적 경사 하강법(stochastic gradient descent)**이라 한다.  \n",
    "딥러닝 프레임 워크는 SGD 함수로 이 기능을 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##미니 배치 학습\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000 #반복횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # 개선판으로, grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니배치 크기 : 100  \n",
    "* 6만개 중 100개(이미지 + 정답 레이블) 임의로 추출\n",
    "* 100개 대상으로 SGD 수행하여 매개변수 갱신  \n",
    "* 경사법 갱신 횟수(반복 횟수) : 10000번 설정\n",
    "* 갱신할 때마다 훈련 데이터 손실 함수 계산 후, 배열에 추가  \n",
    "![img10](./deep_learning_images/fig_4-11.png)\n",
    "\n",
    "학습 횟수가 늘면서 손실 함수 값이 줄어든다. 학습이 잘 되고, 신경망 가중치 매개변수가 서서히 데이터에 적응한다.(= 신경망이 학습한다.)  \n",
    "데이터를 반복해서 학습하여, 최적의 가중치 매개변수로 다가간다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 에폭(epoch) : 하나의 단위로, 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당한다  \n",
    "* 훈련 데이터 10000개를 100개의 미니배치로 학습하면, SGD를 100회 반복했을 때, 훈련 데이터를 모두 '소진'한다.  \n",
    "= 100회가 1 epoch이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시험 데이터로 평가하기\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
    "\n",
    "# 하이퍼 파라미터\n",
    "iters_num = 10000 # 반복횟수 적절히 설정\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # 미니 배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1 에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1) # 둘 중 가장 큰 걸 출력한다?\n",
    "\n",
    "for i in range(iters_num):\n",
    "    #미니배치\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    #기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    #grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"훈련 정확도, 시험 정확도 : \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1에폭(=100회 SGD로 훈련데이터 모두 소진)마다 모든 훈련 데이터와 시험 데이터 정확도 계산하고, 그 결과를 기록한다.  \n",
    "정확도를 1에폭마다 계산 이유 : for 문 안에 매번 계산 시간이 길다. 추이만 알 수 있으면 된다.  \n",
    "\n",
    "![img12](./deep_learning_images/fig_4-12.png)\n",
    "\n",
    "* 훈련 데이터 정확도 실선, 시험 데이터 정확도 점선 표기   \n",
    "* 에폭이 진행될수록, 훈련 데이터와 시험 데이터를 사용하여 평가한 정확도가 모두 좋아진다.  \n",
    "* 두 정확도에 차이가 없다.(= 오버피팅이 일어나지 않았다.)  \n",
    "* 오버피팅 방지를 위해 조기 종료(early stopping)을 권한다. 시험데이터 정확도가 떨어지는 순간 학습을 중단한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "신경망 학습 : 손실 함수라는 지표를 도입  \n",
    "신경망 학습의 목표 : 손실 함수 기준으로 그 값이 가장 작아지는 가중치 매개변수 값을 찾기  \n",
    "경사법 : 함수의 기울기 이용해서, 가능한 작은 손실 함수의 값 찾는 방법  \n",
    "* 요약  \n",
    "* 기계학습에서 사용하는 데이터셋은 _훈련 데이터와 시험 데이터_로 나눠 사용한다.\n",
    "* 훈련 데이터로 학습한 모델의 **범용 능력**을 시험 데이터로 평가한다.\n",
    "* 신경망 학습은 **손실 함수**를 지표로, 손실 함수의 값이 작아지는 방향으로 가중치 매개변수를 갱신한다.\n",
    "* 가중치 매개변수를 갱신할 때는 가중치 매개변수의 **기울기**를 이용하고, 기울어진 방향으로 가중치의 값을 갱신하는 작업을 반복한다.\n",
    "* 아주 작은 값을 주었을 때의 차분으로 미분하는 것을 **수치 미분**이라고 한다.\n",
    "* 수치 미분을 이용해 가중치 매개변수의 기울기를 구할 수 있다.\n",
    "* 수치 미분을 이용한 계산에는 시간이 걸리지만, 그 구현은 간단하다. 한편, 다음 장에서 구현하는 (다소 복잡한) 오차역전파법은 기울기를 고속으로 구할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
