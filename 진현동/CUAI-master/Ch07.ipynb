{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 합성곱 신경망(CNN)\n",
    "## 7.1 전체구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 완전연결 신경망과의 다른점\n",
    "#### Affine -> ReLU : 완전연결 신경망\n",
    "#### Conv -> ReLU -> Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.3 합성곱 계층 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col\n",
    "\n",
    "x1=np.random.rand(1, 3, 7, 7) # input data : 데이터 수: 1, 채널 수 :3, 세로 가로 : 7\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad =0)\n",
    "print(col1.shape)\n",
    "\n",
    "x2 = np.random.rand(10,3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride =1, pad =0)\n",
    "print(col2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "[[[[ 0  1  2]\n",
      "   [ 3  4  5]\n",
      "   [ 6  7  8]]\n",
      "\n",
      "  [[ 9 10 11]\n",
      "   [12 13 14]\n",
      "   [15 16 17]]]]\n",
      "[[[[[[0. 0.]\n",
      "     [0. 0.]]\n",
      "\n",
      "    [[0. 0.]\n",
      "     [0. 0.]]]\n",
      "\n",
      "\n",
      "   [[[0. 0.]\n",
      "     [0. 0.]]\n",
      "\n",
      "    [[0. 0.]\n",
      "     [0. 0.]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[0. 0.]\n",
      "     [0. 0.]]\n",
      "\n",
      "    [[0. 0.]\n",
      "     [0. 0.]]]\n",
      "\n",
      "\n",
      "   [[[0. 0.]\n",
      "     [0. 0.]]\n",
      "\n",
      "    [[0. 0.]\n",
      "     [0. 0.]]]]]]\n",
      "3\n",
      "[[[[ 4  5]\n",
      "   [ 7  8]]\n",
      "\n",
      "  [[13 14]\n",
      "   [16 17]]]]\n",
      "[[[[ 4.  5.]\n",
      "   [ 7.  8.]]\n",
      "\n",
      "  [[13. 14.]\n",
      "   [16. 17.]]]]\n",
      "[[[[[[ 0.  1.]\n",
      "     [ 3.  4.]]\n",
      "\n",
      "    [[ 1.  2.]\n",
      "     [ 4.  5.]]]\n",
      "\n",
      "\n",
      "   [[[ 3.  4.]\n",
      "     [ 6.  7.]]\n",
      "\n",
      "    [[ 4.  5.]\n",
      "     [ 7.  8.]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[ 9. 10.]\n",
      "     [12. 13.]]\n",
      "\n",
      "    [[10. 11.]\n",
      "     [13. 14.]]]\n",
      "\n",
      "\n",
      "   [[[12. 13.]\n",
      "     [15. 16.]]\n",
      "\n",
      "    [[13. 14.]\n",
      "     [16. 17.]]]]]]\n",
      "[[ 0.  1.  3.  4.  9. 10. 12. 13.]\n",
      " [ 1.  2.  4.  5. 10. 11. 13. 14.]\n",
      " [ 3.  4.  6.  7. 12. 13. 15. 16.]\n",
      " [ 4.  5.  7.  8. 13. 14. 16. 17.]]\n"
     ]
    }
   ],
   "source": [
    "H = 3, W= 3,stride=1, pad =0\n",
    "input_data = np.arange(1*2*3*3).reshape((1,2,3,3))\n",
    "filter_h = 2, filter_w = 2\n",
    "N, C, H, W = input_data.shape\n",
    "out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "print(out_h)\n",
    "print(out_w)\n",
    "\n",
    "img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "print(img)\n",
    "col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "print(col)\n",
    "\n",
    "for y in range(filter_h):\n",
    "    y_max = y + stride*out_h\n",
    "    for x in range(filter_w):\n",
    "        x_max = x + stride*out_w\n",
    "        col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "print(y_max)\n",
    "print(img[:, :, y:y_max:stride, x:x_max:stride])\n",
    "print(col[:, :, y, x, :, :])\n",
    "print(col)\n",
    "col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.         0.         0.         0.         0.         0.\n",
      "    0.         0.         0.        ]\n",
      "   [0.         0.32264774 0.84763797 0.94800891 0.59915087 0.83739617\n",
      "    0.64428584 0.87952552 0.        ]\n",
      "   [0.         0.34652863 0.18223796 0.13585619 0.62497646 0.40139099\n",
      "    0.29987387 0.71901111 0.        ]\n",
      "   [0.         0.57007403 0.19231514 0.85397954 0.42195431 0.50007934\n",
      "    0.03669608 0.95872088 0.        ]\n",
      "   [0.         0.40816708 0.72771553 0.2105156  0.1682018  0.5656177\n",
      "    0.9368256  0.10820918 0.        ]\n",
      "   [0.         0.89417535 0.03439244 0.11792981 0.66772536 0.32588668\n",
      "    0.06547063 0.18350178 0.        ]\n",
      "   [0.         0.14335637 0.24369782 0.78760943 0.35135158 0.95880208\n",
      "    0.95884634 0.74233461 0.        ]\n",
      "   [0.         0.37045776 0.64789599 0.37666027 0.76818496 0.17951963\n",
      "    0.98928655 0.89746357 0.        ]\n",
      "   [0.         0.         0.         0.         0.         0.\n",
      "    0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.         0.\n",
      "    0.         0.         0.        ]\n",
      "   [0.         0.76188642 0.62990582 0.79825572 0.39646978 0.19894111\n",
      "    0.90056032 0.23280867 0.        ]\n",
      "   [0.         0.43892965 0.98341021 0.58037902 0.59381554 0.64266948\n",
      "    0.43944603 0.12912331 0.        ]\n",
      "   [0.         0.07962394 0.49270936 0.24486551 0.47567488 0.88695854\n",
      "    0.08395752 0.91526225 0.        ]\n",
      "   [0.         0.60406214 0.16724983 0.17071576 0.37411786 0.21514956\n",
      "    0.41778579 0.74147847 0.        ]\n",
      "   [0.         0.17284201 0.77443631 0.64720329 0.90205464 0.65449832\n",
      "    0.48426067 0.78636962 0.        ]\n",
      "   [0.         0.54815466 0.20122783 0.45546089 0.4124379  0.27611022\n",
      "    0.47711503 0.77641127 0.        ]\n",
      "   [0.         0.68296049 0.59718538 0.06943923 0.31725935 0.68479941\n",
      "    0.90743473 0.17131658 0.        ]\n",
      "   [0.         0.         0.         0.         0.         0.\n",
      "    0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.         0.         0.\n",
      "    0.         0.         0.        ]\n",
      "   [0.         0.11355342 0.54734037 0.01994589 0.78737957 0.37836813\n",
      "    0.7085418  0.95746254 0.        ]\n",
      "   [0.         0.76138368 0.15544569 0.32946657 0.53764103 0.10549588\n",
      "    0.17256574 0.85771722 0.        ]\n",
      "   [0.         0.8452987  0.29292088 0.09030819 0.35728458 0.58361362\n",
      "    0.49636439 0.82450361 0.        ]\n",
      "   [0.         0.33533308 0.58273091 0.9288912  0.97153876 0.81121806\n",
      "    0.15828238 0.76988269 0.        ]\n",
      "   [0.         0.35231488 0.41932835 0.77727738 0.79993116 0.26728932\n",
      "    0.77807673 0.33601121 0.        ]\n",
      "   [0.         0.73362514 0.32392554 0.08827589 0.11188608 0.15723528\n",
      "    0.90376609 0.52837696 0.        ]\n",
      "   [0.         0.4722708  0.15591234 0.45117105 0.86632107 0.83514372\n",
      "    0.26477071 0.58548928 0.        ]\n",
      "   [0.         0.         0.         0.         0.         0.\n",
      "    0.         0.         0.        ]]]]\n"
     ]
    }
   ],
   "source": [
    " img = np.pad(x1, [(0,0), (0,0), (1, 1), (1, 1)], 'constant')\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride =1, pad =0):\n",
    "        self.W = W\n",
    "        self.b = b \n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1+(H+2*self.pad -FH) / self.stride)\n",
    "        out_w = int(1+(H+2*self.pad - FW)/ self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1)\n",
    "        out = np. dot(col,col_W) +self.b\n",
    "        \n",
    "        out = out.reshape(N,out_h, out_w, -1).transpose(0,3,1,2)\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위 class에서 사용된 것\n",
    "### transpose; 2차원배열의 transpose는 아주 쉽지만 2 이상의 차원에서의 transpose는 생각하기어렵다.\n",
    "### Ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1  5  9]\n",
      "  [ 2  6 10]]\n",
      "\n",
      " [[ 3  7 11]\n",
      "  [ 4  8 12]]]\n",
      "[[[ 1  2]\n",
      "  [ 3  4]]\n",
      "\n",
      " [[ 5  6]\n",
      "  [ 7  8]]\n",
      "\n",
      " [[ 9 10]\n",
      "  [11 12]]]\n",
      "[[[ 1  3]\n",
      "  [ 2  4]]\n",
      "\n",
      " [[ 5  7]\n",
      "  [ 6  8]]\n",
      "\n",
      " [[ 9 11]\n",
      "  [10 12]]]\n"
     ]
    }
   ],
   "source": [
    "c =np.array([[[1,5,9], [2,6,10]], [[3,7,11], [4,8,12]]])\n",
    "print(c)\n",
    "k = c.transpose(2,0,1)\n",
    "print(k)\n",
    "l = c.transpose(2,1,0)\n",
    "print(l)\n",
    "# 0차원 원소 3개씩 2개로 이루어진 1차원 2개 \n",
    "# 1차원이 3개로,,, 원소 수 는 2개로 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3], [4,5,6]]) #2차원 배열\n",
    "b = a.transpose(1,0)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride =1, pad =0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad= pad\n",
    "        \n",
    "    def forward(self,x):\n",
    "        N,C,H,W = x.shape\n",
    "        out_h = int(1+(H-self.pool_h)/ self.stride)\n",
    "        out_w = int(1+(h-self.pool_w)/ self.stride)\n",
    "        \n",
    "        # 입력데이터를 전개\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self,pad)\n",
    "        col = col.reshape(-1, self,pool_h * self.pool_w)\n",
    "        # \"행\"별 최댓값 찾기 ( axis =1)\n",
    "        out = np.max(col, axis=1)\n",
    "        # 적절한 모양으로 reshape\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0,3,1,2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 CNN구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNN의 네트워크는 convolution > ReLU > pooling > conv> ... > Affine > ReLU > Affine > Softmax\n",
    "- input_dim: 입력데이터의 차원 (채널 수, 높이, 너비)\n",
    "- conv_param: 하이퍼파라미터(딕셔너리). 딕셔너리의 키는 필터 수, 필터 크기, 스트라이드, 은닉층 수, 출력층 수, 초기화시 가중치 표준편차.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    def forward(self,x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] =0\n",
    "        return out\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] =0\n",
    "        dx = dout\n",
    "        return dx\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), conv_param={'filter_num': 30, 'filter_size':5, 'pad':0, 'stride':1}, hidden_size = 100, output_size =10, weight_init_std = 0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride +1 \n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) *(conv_output_size/2))\n",
    "        \n",
    "        # 가중치 매개변수 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        #CNN을 구성하는 계층들을 생성한다. \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['ReLU1'] = ReLU()\n",
    "        self.layers['Pool1'] = Pooling(pool_h =2, pool_w =2, stride =2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['ReLU'] = ReLU()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    def loss(self, x, t):\n",
    "        y= self.predict(x)\n",
    "        return self.last_layer.forward(y,t)\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x,t)\n",
    "        \n",
    "        dout =1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        grads ={}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    ## 책 말고 필요한 함수가 많음\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]\n",
    "            \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3003629085034465\n",
      "=== epoch:1, train acc:0.239, test acc:0.228 ===\n",
      "train loss:2.2979776976870196\n",
      "train loss:2.2956214834989153\n",
      "train loss:2.290254377796724\n",
      "train loss:2.282980761786826\n",
      "train loss:2.2764957528020573\n",
      "train loss:2.2614345611089486\n",
      "train loss:2.2262899664037463\n",
      "train loss:2.223473246142177\n",
      "train loss:2.2235692634122977\n",
      "train loss:2.19250943978051\n",
      "train loss:2.1475014318281525\n",
      "train loss:2.1168334478709405\n",
      "train loss:2.0413216865155963\n",
      "train loss:1.997057016732293\n",
      "train loss:1.9856034443171515\n",
      "train loss:1.9156820727085306\n",
      "train loss:1.840516877533316\n",
      "train loss:1.7674973134149101\n",
      "train loss:1.6965087892704316\n",
      "train loss:1.6338623094848628\n",
      "train loss:1.4221349120230462\n",
      "train loss:1.416214950519128\n",
      "train loss:1.3280232062902255\n",
      "train loss:1.2330316904454823\n",
      "train loss:1.2407946015180253\n",
      "train loss:1.0583588325382056\n",
      "train loss:0.9341534581936959\n",
      "train loss:0.976996627068311\n",
      "train loss:0.9066706167097125\n",
      "train loss:0.9562720184600493\n",
      "train loss:0.8778361073902957\n",
      "train loss:0.8370855342213727\n",
      "train loss:0.9047334124844507\n",
      "train loss:0.8273227725122981\n",
      "train loss:0.7272645864372784\n",
      "train loss:0.6584363766505957\n",
      "train loss:0.721365450868282\n",
      "train loss:0.7224170215780539\n",
      "train loss:0.7204252412546347\n",
      "train loss:0.6497851173350107\n",
      "train loss:0.7285643399434959\n",
      "train loss:0.6673722794557245\n",
      "train loss:0.5947195119652215\n",
      "train loss:0.5721692843219781\n",
      "train loss:0.72416647631477\n",
      "train loss:0.7862525600296273\n",
      "train loss:0.6863423712392048\n",
      "train loss:0.779223200150627\n",
      "train loss:0.5503803913151117\n",
      "train loss:0.5110712140312134\n",
      "train loss:0.4258335095191467\n",
      "train loss:0.6164094970642213\n",
      "train loss:0.6627556558138906\n",
      "train loss:0.6870540539324455\n",
      "train loss:0.6285449965301997\n",
      "train loss:0.4205278954472263\n",
      "train loss:0.6243702983273999\n",
      "train loss:0.42955601820843625\n",
      "train loss:0.610379474942764\n",
      "train loss:0.4162270887494243\n",
      "train loss:0.49112623982622006\n",
      "train loss:0.44356877372100045\n",
      "train loss:0.4742536032075773\n",
      "train loss:0.6186826259795478\n",
      "train loss:0.6812484780525391\n",
      "train loss:0.5227268656805202\n",
      "train loss:0.6058950771245428\n",
      "train loss:0.5062152741195315\n",
      "train loss:0.5883930359801347\n",
      "train loss:0.5088944206413332\n",
      "train loss:0.4616560524538941\n",
      "train loss:0.48327484399319437\n",
      "train loss:0.47440392409542875\n",
      "train loss:0.37131862349072486\n",
      "train loss:0.45163096580266393\n",
      "train loss:0.47581013340789147\n",
      "train loss:0.520828428418423\n",
      "train loss:0.37677884164645703\n",
      "train loss:0.4922235296414144\n",
      "train loss:0.3564481387719908\n",
      "train loss:0.5270085414518847\n",
      "train loss:0.5131389126231641\n",
      "train loss:0.43316307476615956\n",
      "train loss:0.5500684097107448\n",
      "train loss:0.4952643342277146\n",
      "train loss:0.45802007211567075\n",
      "train loss:0.4056977514459677\n",
      "train loss:0.23670868946383208\n",
      "train loss:0.4413107675340368\n",
      "train loss:0.290699426035767\n",
      "train loss:0.46258044868376463\n",
      "train loss:0.41084105092297285\n",
      "train loss:0.4105323232516282\n",
      "train loss:0.44201377715429\n",
      "train loss:0.44081083866389437\n",
      "train loss:0.535051454558089\n",
      "train loss:0.51146665007379\n",
      "train loss:0.41514897309503107\n",
      "train loss:0.2810291697329939\n",
      "train loss:0.3070120566568619\n",
      "train loss:0.5262650641998122\n",
      "train loss:0.3417254691324528\n",
      "train loss:0.26958224254695856\n",
      "train loss:0.33375712564861465\n",
      "train loss:0.41768627854745793\n",
      "train loss:0.374296844677967\n",
      "train loss:0.4645811109358522\n",
      "train loss:0.32802608323628896\n",
      "train loss:0.38339736919564826\n",
      "train loss:0.4595872027510661\n",
      "train loss:0.4511597930540823\n",
      "train loss:0.46976312420552574\n",
      "train loss:0.4306102420470556\n",
      "train loss:0.619408198907891\n",
      "train loss:0.2877871282566307\n",
      "train loss:0.3715390383219122\n",
      "train loss:0.373584620783196\n",
      "train loss:0.2473097076636446\n",
      "train loss:0.501570763223247\n",
      "train loss:0.36520012031086446\n",
      "train loss:0.29794838463990625\n",
      "train loss:0.3411814293429111\n",
      "train loss:0.34733091590600884\n",
      "train loss:0.39589148919875583\n",
      "train loss:0.2729317583009869\n",
      "train loss:0.4045928680981781\n",
      "train loss:0.4348671076300545\n",
      "train loss:0.38831312339561275\n",
      "train loss:0.3357998153034788\n",
      "train loss:0.20414776319562425\n",
      "train loss:0.39854857981104536\n",
      "train loss:0.41217234514319\n",
      "train loss:0.4426088049771347\n",
      "train loss:0.29003802377656857\n",
      "train loss:0.25554882252844247\n",
      "train loss:0.35399120722844635\n",
      "train loss:0.41582589781378054\n",
      "train loss:0.31069447565703323\n",
      "train loss:0.42404637649218174\n",
      "train loss:0.3117949079731763\n",
      "train loss:0.22330463134699358\n",
      "train loss:0.3760231334137672\n",
      "train loss:0.3520436159758144\n",
      "train loss:0.5180872590955697\n",
      "train loss:0.3491931659264719\n",
      "train loss:0.3932840099174389\n",
      "train loss:0.31683112340021385\n",
      "train loss:0.38211859775806295\n",
      "train loss:0.4634380674381671\n",
      "train loss:0.3807685312253698\n",
      "train loss:0.3430894018000049\n",
      "train loss:0.30155737175267455\n",
      "train loss:0.3184427475637825\n",
      "train loss:0.30309753183738586\n",
      "train loss:0.2080466841251204\n",
      "train loss:0.5522614621424858\n",
      "train loss:0.23095868539052186\n",
      "train loss:0.27848954022647726\n",
      "train loss:0.44181988834685804\n",
      "train loss:0.3167983203581951\n",
      "train loss:0.36902549534279916\n",
      "train loss:0.4595791734254055\n",
      "train loss:0.2990115891855145\n",
      "train loss:0.42438828264586337\n",
      "train loss:0.3276236897751816\n",
      "train loss:0.26490150054776906\n",
      "train loss:0.2701154016481938\n",
      "train loss:0.5397153273931731\n",
      "train loss:0.19614935548329204\n",
      "train loss:0.29046830230750487\n",
      "train loss:0.21665104657800097\n",
      "train loss:0.27150434061215956\n",
      "train loss:0.4139158879215624\n",
      "train loss:0.34328504589292075\n",
      "train loss:0.18661310257934005\n",
      "train loss:0.3224903394261203\n",
      "train loss:0.4077417765170999\n",
      "train loss:0.20577703729771923\n",
      "train loss:0.18437297627786672\n",
      "train loss:0.357354898713326\n",
      "train loss:0.3276544452454931\n",
      "train loss:0.288954668337058\n",
      "train loss:0.2583659927403894\n",
      "train loss:0.22606201723659577\n",
      "train loss:0.27263692400257766\n",
      "train loss:0.2711260946916262\n",
      "train loss:0.29962594893504013\n",
      "train loss:0.29806054188680087\n",
      "train loss:0.27055422294856374\n",
      "train loss:0.37686846565863297\n",
      "train loss:0.370266326002081\n",
      "train loss:0.2971999683526558\n",
      "train loss:0.21961800989154992\n",
      "train loss:0.1819147605558465\n",
      "train loss:0.3678599942189966\n",
      "train loss:0.1326673032891425\n",
      "train loss:0.2515797651244877\n",
      "train loss:0.2610275776167173\n",
      "train loss:0.16064169156546038\n",
      "train loss:0.21866443078345266\n",
      "train loss:0.2632519234078094\n",
      "train loss:0.26501642683381293\n",
      "train loss:0.2789734109662353\n",
      "train loss:0.2873000116265526\n",
      "train loss:0.2736566893532217\n",
      "train loss:0.2963738668031855\n",
      "train loss:0.3845390563244221\n",
      "train loss:0.3201152268830317\n",
      "train loss:0.3431775931626299\n",
      "train loss:0.3512607093744791\n",
      "train loss:0.2566249919978985\n",
      "train loss:0.19940888366898515\n",
      "train loss:0.3011514424005615\n",
      "train loss:0.24764558493427885\n",
      "train loss:0.2869997194854983\n",
      "train loss:0.22871441879324655\n",
      "train loss:0.33632215914051344\n",
      "train loss:0.29452280643491796\n",
      "train loss:0.2373735107326538\n",
      "train loss:0.23736607238481294\n",
      "train loss:0.2642825912199495\n",
      "train loss:0.23188495805624007\n",
      "train loss:0.259864510675605\n",
      "train loss:0.2818795640388342\n",
      "train loss:0.23218270359960363\n",
      "train loss:0.20052966424821758\n",
      "train loss:0.17962706015419408\n",
      "train loss:0.21191428111768476\n",
      "train loss:0.35622221204270443\n",
      "train loss:0.3066732464949766\n",
      "train loss:0.18236585743327893\n",
      "train loss:0.15072481660340678\n",
      "train loss:0.18587958679625483\n",
      "train loss:0.19115011338590404\n",
      "train loss:0.2669397166805597\n",
      "train loss:0.21756466922775236\n",
      "train loss:0.3221814201480512\n",
      "train loss:0.2630901976242623\n",
      "train loss:0.19182235483127094\n",
      "train loss:0.16330853724805217\n",
      "train loss:0.21646197384412053\n",
      "train loss:0.26680409452530485\n",
      "train loss:0.25349498430703893\n",
      "train loss:0.2803646478658629\n",
      "train loss:0.30101701917794677\n",
      "train loss:0.42512690541258685\n",
      "train loss:0.33495748021927246\n",
      "train loss:0.2110165907920369\n",
      "train loss:0.22462398129581584\n",
      "train loss:0.1401335321702099\n",
      "train loss:0.2674585691913735\n",
      "train loss:0.31839813146296997\n",
      "train loss:0.3839713374406591\n",
      "train loss:0.3532268406011065\n",
      "train loss:0.4111764052896196\n",
      "train loss:0.33022596090030154\n",
      "train loss:0.28274714847878096\n",
      "train loss:0.15772912096101735\n",
      "train loss:0.24564049236342395\n",
      "train loss:0.2324199260051271\n",
      "train loss:0.20098786580802536\n",
      "train loss:0.29911141642725053\n",
      "train loss:0.23062685866707927\n",
      "train loss:0.2829587702210117\n",
      "train loss:0.3591874534960112\n",
      "train loss:0.19047036191325972\n",
      "train loss:0.589504115984676\n",
      "train loss:0.1720035471983034\n",
      "train loss:0.20940939786172108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.2302872730720763\n",
      "train loss:0.2237848752272504\n",
      "train loss:0.23521460546675876\n",
      "train loss:0.2574745605647042\n",
      "train loss:0.11593544419153917\n",
      "train loss:0.2508492323439637\n",
      "train loss:0.31660735178719246\n",
      "train loss:0.17755758308099676\n",
      "train loss:0.3034242261169522\n",
      "train loss:0.176560869316022\n",
      "train loss:0.244376032182736\n",
      "train loss:0.29952187346604\n",
      "train loss:0.3373449241257241\n",
      "train loss:0.16822916871201116\n",
      "train loss:0.2209282997953477\n",
      "train loss:0.19186678176856625\n",
      "train loss:0.23315922152054733\n",
      "train loss:0.28556214218854786\n",
      "train loss:0.21259446978867685\n",
      "train loss:0.25615903440680865\n",
      "train loss:0.18730372443821622\n",
      "train loss:0.29151361838170803\n",
      "train loss:0.19456886276055024\n",
      "train loss:0.2371059980880174\n",
      "train loss:0.21204321350659858\n",
      "train loss:0.2006682818319074\n",
      "train loss:0.2540807337325415\n",
      "train loss:0.07490287689937439\n",
      "train loss:0.18816504797844558\n",
      "train loss:0.22220044933624816\n",
      "train loss:0.3136887432247311\n",
      "train loss:0.21997064774084749\n",
      "train loss:0.22432884647913734\n",
      "train loss:0.21622677907376314\n",
      "train loss:0.22048402963487967\n",
      "train loss:0.24891015467436495\n",
      "train loss:0.2043402423512618\n",
      "train loss:0.23448247664747765\n",
      "train loss:0.28168649799933954\n",
      "train loss:0.2747966783373703\n",
      "train loss:0.33407818316698945\n",
      "train loss:0.44346838369627134\n",
      "train loss:0.21183082302672293\n",
      "train loss:0.20717409055936037\n",
      "train loss:0.26065258400554064\n",
      "train loss:0.19167150010258574\n",
      "train loss:0.16639040894189708\n",
      "train loss:0.22295600352553296\n",
      "train loss:0.2060232894044112\n",
      "train loss:0.2928383368865894\n",
      "train loss:0.2173987335496877\n",
      "train loss:0.2712714443479446\n",
      "train loss:0.18527496319283526\n",
      "train loss:0.20751540177767702\n",
      "train loss:0.2272412884850012\n",
      "train loss:0.20782969580099614\n",
      "train loss:0.16330351207107782\n",
      "train loss:0.18861850731689841\n",
      "train loss:0.20670225060984135\n",
      "train loss:0.1630414625553779\n",
      "train loss:0.13695504751929907\n",
      "train loss:0.09196770096570749\n",
      "train loss:0.1583735483322691\n",
      "train loss:0.1828221414416985\n",
      "train loss:0.24259979103963336\n",
      "train loss:0.15270079930939937\n",
      "train loss:0.17752286636829326\n",
      "train loss:0.09577031910157902\n",
      "train loss:0.20324867644016156\n",
      "train loss:0.09195136997365744\n",
      "train loss:0.17134009389579144\n",
      "train loss:0.20447677339314194\n",
      "train loss:0.22698330381820658\n",
      "train loss:0.16701494776823025\n",
      "train loss:0.2713197075643203\n",
      "train loss:0.15111940507010352\n",
      "train loss:0.16599001018859055\n",
      "train loss:0.1238131905484922\n",
      "train loss:0.1569581817424687\n",
      "train loss:0.16105123314151024\n",
      "train loss:0.1830657630043609\n",
      "train loss:0.060584337406750476\n",
      "train loss:0.22516129797903944\n",
      "train loss:0.1813293894315297\n",
      "train loss:0.15549647092212235\n",
      "train loss:0.06478248602293513\n",
      "train loss:0.22887472649357513\n",
      "train loss:0.10003133290937889\n",
      "train loss:0.3093626899460056\n",
      "train loss:0.167820192429913\n",
      "train loss:0.2706203685790242\n",
      "train loss:0.188143432849615\n",
      "train loss:0.22493522525059784\n",
      "train loss:0.11473019677914045\n",
      "train loss:0.15306531618266944\n",
      "train loss:0.26265973593533687\n",
      "train loss:0.2337153610086749\n",
      "train loss:0.1753187716952974\n",
      "train loss:0.23363752029640333\n",
      "train loss:0.23026503088047587\n",
      "train loss:0.29203794171995073\n",
      "train loss:0.06732599799855912\n",
      "train loss:0.21170317094101498\n",
      "train loss:0.18529653849058186\n",
      "train loss:0.11620028208846879\n",
      "train loss:0.16957830862689274\n",
      "train loss:0.27746143462056944\n",
      "train loss:0.1875066639677523\n",
      "train loss:0.10413819763621414\n",
      "train loss:0.2926640753783033\n",
      "train loss:0.12216345621155122\n",
      "train loss:0.056112923964789486\n",
      "train loss:0.1279333601988893\n",
      "train loss:0.15309418092053279\n",
      "train loss:0.1779697592828141\n",
      "train loss:0.2091800063801636\n",
      "train loss:0.16413212435369612\n",
      "train loss:0.24228194151041582\n",
      "train loss:0.20831890041596643\n",
      "train loss:0.20426682792069578\n",
      "train loss:0.20907588889911094\n",
      "train loss:0.10915490360180483\n",
      "train loss:0.17125858717326903\n",
      "train loss:0.08895438261965777\n",
      "train loss:0.14515674086745223\n",
      "train loss:0.22277910304424978\n",
      "train loss:0.2697803941186257\n",
      "train loss:0.17103975863345597\n",
      "train loss:0.2010653582786222\n",
      "train loss:0.19482087845361346\n",
      "train loss:0.20506646735593748\n",
      "train loss:0.21544723489515052\n",
      "train loss:0.1463415637656826\n",
      "train loss:0.10963253326789023\n",
      "train loss:0.156461001971944\n",
      "train loss:0.11089289611258556\n",
      "train loss:0.1651614725685689\n",
      "train loss:0.21019595844160324\n",
      "train loss:0.17090219426973582\n",
      "train loss:0.1300859057602652\n",
      "train loss:0.13048194058986173\n",
      "train loss:0.128874920361961\n",
      "train loss:0.1737406194799265\n",
      "train loss:0.10632212395062751\n",
      "train loss:0.18167830587451797\n",
      "train loss:0.24654373845084063\n",
      "train loss:0.18259939196191963\n",
      "train loss:0.08196861104942739\n",
      "train loss:0.18636980104172055\n",
      "train loss:0.14308273214379175\n",
      "train loss:0.25110813305930485\n",
      "train loss:0.31956137142920943\n",
      "train loss:0.3590073082724868\n",
      "train loss:0.10960728140088605\n",
      "train loss:0.12452965320259772\n",
      "train loss:0.15984486641960788\n",
      "train loss:0.1828817131106436\n",
      "train loss:0.11995014215268109\n",
      "train loss:0.0981760093628676\n",
      "train loss:0.2042923563400153\n",
      "train loss:0.17191765436947254\n",
      "train loss:0.16369114066712814\n",
      "train loss:0.1860434783732799\n",
      "train loss:0.1279921281392376\n",
      "train loss:0.13676828972712363\n",
      "train loss:0.28528853838298973\n",
      "train loss:0.1582204816418955\n",
      "train loss:0.12790013410513873\n",
      "train loss:0.1193956675265881\n",
      "train loss:0.14609085142826386\n",
      "train loss:0.17234986560656085\n",
      "train loss:0.12851192495239894\n",
      "train loss:0.15001406773257536\n",
      "train loss:0.134916302861966\n",
      "train loss:0.19263067673637158\n",
      "train loss:0.1571399947753157\n",
      "train loss:0.108517473074499\n",
      "train loss:0.17861963405272058\n",
      "train loss:0.11322235552260787\n",
      "train loss:0.15098512544783524\n",
      "train loss:0.2325372740695244\n",
      "train loss:0.11607390352943626\n",
      "train loss:0.10472987612619906\n",
      "train loss:0.1771553170468664\n",
      "train loss:0.11667170043158491\n",
      "train loss:0.20660156251006584\n",
      "train loss:0.10426444446787184\n",
      "train loss:0.13825987901888892\n",
      "train loss:0.22483256475668825\n",
      "train loss:0.11843057210923574\n",
      "train loss:0.14095505956862117\n",
      "train loss:0.09405454239077116\n",
      "train loss:0.10294639482029727\n",
      "train loss:0.15291278043844397\n",
      "train loss:0.20285257963496633\n",
      "train loss:0.0704709860346045\n",
      "train loss:0.136955254483475\n",
      "train loss:0.24906707125627484\n",
      "train loss:0.1718492274073218\n",
      "train loss:0.14080473809266766\n",
      "train loss:0.1530242473004951\n",
      "train loss:0.08566337946129847\n",
      "train loss:0.1551866840574995\n",
      "train loss:0.14679842006094443\n",
      "train loss:0.07115986921522274\n",
      "train loss:0.1147260074845115\n",
      "train loss:0.2371341994705176\n",
      "train loss:0.07977761727931591\n",
      "train loss:0.13571261609440058\n",
      "train loss:0.1330318214979366\n",
      "train loss:0.13840205439314554\n",
      "train loss:0.07661646096107075\n",
      "train loss:0.14444242153946232\n",
      "train loss:0.044910446064068264\n",
      "train loss:0.2207038735407433\n",
      "train loss:0.11842266124206707\n",
      "train loss:0.18199030069186972\n",
      "train loss:0.07065615258798624\n",
      "train loss:0.22957928535214203\n",
      "train loss:0.05364960621059299\n",
      "train loss:0.20885132631215161\n",
      "train loss:0.28361000526999813\n",
      "train loss:0.070550334461509\n",
      "train loss:0.13280580706334094\n",
      "train loss:0.06647602395823052\n",
      "train loss:0.15172665248283712\n",
      "train loss:0.0975504212376633\n",
      "train loss:0.11162563183128492\n",
      "train loss:0.12130522257940538\n",
      "train loss:0.10970626714538131\n",
      "train loss:0.11803385734273668\n",
      "train loss:0.18055623683463903\n",
      "train loss:0.13158863288450942\n",
      "train loss:0.10619025851192192\n",
      "train loss:0.08431189325580599\n",
      "train loss:0.22198633093437276\n",
      "train loss:0.0864891071038595\n",
      "train loss:0.17528836621179075\n",
      "train loss:0.1497737796348909\n",
      "train loss:0.23480900644500077\n",
      "train loss:0.15683359830691035\n",
      "train loss:0.15688451234296732\n",
      "train loss:0.06492478110639271\n",
      "train loss:0.11986701336662342\n",
      "train loss:0.09193941700966227\n",
      "train loss:0.09785370324520598\n",
      "train loss:0.13519507471934744\n",
      "train loss:0.15483176640006546\n",
      "train loss:0.19020986974233073\n",
      "train loss:0.0685299893767015\n",
      "train loss:0.18415223140050452\n",
      "train loss:0.07549033394154923\n",
      "train loss:0.09767149030205916\n",
      "train loss:0.15612149404892187\n",
      "train loss:0.11106675593700457\n",
      "train loss:0.07043256230382704\n",
      "train loss:0.09796780610864733\n",
      "train loss:0.05779181826038487\n",
      "train loss:0.10833529051135926\n",
      "train loss:0.19070995866341758\n",
      "train loss:0.1602441738743885\n",
      "train loss:0.13770566390831515\n",
      "train loss:0.1732528898699083\n",
      "train loss:0.0700070194022258\n",
      "train loss:0.21054366205910002\n",
      "train loss:0.11148999963407003\n",
      "train loss:0.0764500150091351\n",
      "train loss:0.14349581086987864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.11675967720485732\n",
      "train loss:0.12600510956401642\n",
      "train loss:0.04390722753137429\n",
      "train loss:0.10396017445252415\n",
      "train loss:0.17865839964259286\n",
      "train loss:0.14243147243191215\n",
      "train loss:0.07510518712337051\n",
      "train loss:0.118848497545165\n",
      "train loss:0.13930219876118527\n",
      "train loss:0.15289625737870696\n",
      "train loss:0.04839694735318988\n",
      "train loss:0.18762418027186623\n",
      "train loss:0.09977642690783155\n",
      "train loss:0.14047869194108395\n",
      "train loss:0.054083244874698726\n",
      "train loss:0.10773054878223583\n",
      "train loss:0.055104849472766636\n",
      "train loss:0.13150383434269114\n",
      "train loss:0.14587702262756377\n",
      "train loss:0.117592398822368\n",
      "train loss:0.1030351933641028\n",
      "train loss:0.11674618296341059\n",
      "train loss:0.09229441174398043\n",
      "train loss:0.11624067893613932\n",
      "train loss:0.08183236923252755\n",
      "train loss:0.10634034012502465\n",
      "train loss:0.04972175267511817\n",
      "train loss:0.06737971344156596\n",
      "train loss:0.08889825996941993\n",
      "train loss:0.1220864714946534\n",
      "train loss:0.0886183108776462\n",
      "train loss:0.17625629052883865\n",
      "train loss:0.08578327521895934\n",
      "train loss:0.12944280854843054\n",
      "train loss:0.17661967738573858\n",
      "train loss:0.0809427924897816\n",
      "train loss:0.06008211869242098\n",
      "train loss:0.09912971474646337\n",
      "train loss:0.06480184089368046\n",
      "train loss:0.1821672949157359\n",
      "train loss:0.11052369762457288\n",
      "train loss:0.10899891549527642\n",
      "train loss:0.16215359651071842\n",
      "train loss:0.05621463711426438\n",
      "train loss:0.21034274330016417\n",
      "train loss:0.1371070707420906\n",
      "train loss:0.1717397575699028\n",
      "train loss:0.15825669694852743\n",
      "train loss:0.2035810365039297\n",
      "train loss:0.13323795810751926\n",
      "train loss:0.07008238405149228\n",
      "train loss:0.19361379017431787\n",
      "train loss:0.18389988344124092\n",
      "train loss:0.10506046173385063\n",
      "train loss:0.09863611894402585\n",
      "train loss:0.21638961881209945\n",
      "train loss:0.10986796510872873\n",
      "train loss:0.061982484196204286\n",
      "train loss:0.05625909993557077\n",
      "train loss:0.11968825953019896\n",
      "train loss:0.1071850980491245\n",
      "train loss:0.07786677695392662\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9678\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 1 #원래 설정된 값은 20. 컴터 터질것같아서 1로 수정\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
