{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.1 곱셉 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer :\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x*y\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = dout *self.y\n",
    "        dy = dout*self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "apple =100\n",
    "apple_num = 2\n",
    "tax =1.1\n",
    "\n",
    "#계층설정\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "#순전파\n",
    "apple_price = mul_apple_layer.forward(apple,apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2\n",
      "110.00000000000001\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "#역전파(미분)\n",
    "dprice =1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple)\n",
    "print(dapple_num)\n",
    "print(dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.2 덧셈계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    # 덧셈 계층에서는 초기화가 필요없다. \n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x+y\n",
    "        return x+y\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = dout *1\n",
    "        dy = dout*1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#그림 5.17 (p.163)\n",
    "\n",
    "apple=100\n",
    "apple_num = 2\n",
    "orange =150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "#계층 설정\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "#순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price  = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "# 역전파\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price  = add_apple_orange_layer.backward(dall_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "dorange, dorange_num = mul_apple_layer.backward(dorange_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2\n",
      "2.2 110.00000000000001\n",
      "650\n"
     ]
    }
   ],
   "source": [
    "print(price)\n",
    "print(dapple_num, dapple)\n",
    "print(dorange, dorange_num)\n",
    "print(dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5 활성화 계층 구현하기(ReLU, sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.1 ReLU 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx\n",
    "    \n",
    "x = np.array([[10., -0.5], [-2.0, 3.0]])\n",
    "print(x)\n",
    "\n",
    "mask = (x<=0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.2 Sigmoid 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1+np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 -self.out) *self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.6 Affine, Softmax 계층 구현\n",
    "## 5.6.1 Affine 계층 \n",
    " - 신경망의 순전파에서 수행하는 행렬의 곱은 기하학에서 Affine transformation 이라고 한다. > affine trans. 를하는 처리를 affine 계층이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "X= np.random.rand(2)\n",
    "W = np.random.rand(2,3)\n",
    "B = np.random.rand(3)\n",
    "\n",
    "print(X.shape)\n",
    "print(W.shape)\n",
    "print(B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6.2 배치용 Affine 계층\n",
    "- N개의 데이터를 묶어 순전파 하는경우. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "X_dot_W = np.array([[0,0,0], [10, 10, 10]])\n",
    "B = np.array([1,2,3])\n",
    "\n",
    "print(X_dot_W)\n",
    "print(X_dot_W + B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "dY = np.array([[1,2,3], [4,5,6]])\n",
    "print(dY)\n",
    "\n",
    "dB = np.sum(dY, axis = 0)\n",
    "print(dB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init(self, W, b):\n",
    "        self.W =W\n",
    "        self,b =b\n",
    "        self.x= None\n",
    "        self.dW = None\n",
    "        slef.df = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return  out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.dot(dout, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6.3 Softmax with Loss 계층 \n",
    "- 신경망에서 수행하는 작업은 학습, 추론. 추론 시 일반적으로 softmax계층을 사용하지 않는다. \n",
    "- 신경망의 추론은 마지막 affine 계층의 출력을 인식결과로 이용.\n",
    "- 정규화하지 않은 출력결과를 점수라고 한다. \n",
    "- 신경망 추론에서 답을 1개만 내는 경우에는 softmax 계층 불필요. 하지만 신경망 학습에는 softmax가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t=t\n",
    "        self.y=softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self,t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout =1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y- self.t) /batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.7 오차역전파법 구현하기\n",
    "- 전제: 신경망에는 적응가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈현 데이터에 적응하도록 조정하는 과정을  '학습'이라고 한다.\n",
    "- 1단계: 미니배치 >> 훈련 데이터중 일부를 무작위로 가져온다. 선별한 데이터를 미니배치라고 하며, 이 미니배치의 손실 함수 값을 줄이는 것이 목표\n",
    "- 2단계: 기울기 산출 >> 미니배치의 손실함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실함수의 값을 가장 작게. \n",
    "- 3단계: 가중치 매개변수를 기울기 방향으로 아주 조금 갱신\n",
    "- 4단계: 1~3단계를 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 layer로 구현을 할것이다.\n",
    "#### 인스턴스 변수: params, layers, lastLayer(softmax)\n",
    "#### 클래스의 매서드: __init__, predict, loss, accuracy, numerical_gradient, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.rand(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        \n",
    "        self.params['W2'] = weight_init_std * np.random.rand(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        #계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['ReLu1'] =  Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self,x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "           \n",
    "    def loss(self, x, t):\n",
    "        y= self.predict(x)\n",
    "        return self.lastLayer.forward(y,t)\n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        y =np.argmax(y, axis=1)\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y==t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self,x,t):\n",
    "        loss_W = lambda W: self.loss(x,t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self,x,t):\n",
    "        # 순전파\n",
    "        self.loss(x,t)\n",
    "        \n",
    "        #역전파\n",
    "        dout=1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # 결과 저장. \n",
    "        grads= {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 계층을 OderedDict에 보관하는것이 중요하다.\n",
    " - OrderedDict는 순서가 있는 딕셔너리이다. 딕셔너리에 추가한 순서를 기억하게 하는것. >> 순전파 때는 추가한 순서대로 각 계층의 forward를 호출하면 된다.\n",
    " - 마찬가지로 역전파때는 계층을 반대 순서로 호출하기만 하면 된다. \n",
    " \n",
    "### 신경망의 구성요들을 계층으로 모듈화 하여 구현하는 것은 아주 좋다. \n",
    " - 더 깊은 신경망을 만들때 단순히 필요한 만큼의 계층을 추가하면 되기 때문이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:2.2486547439350546e-10\n",
      "b1:1.0805058431935956e-09\n",
      "W2:7.21377318080585e-08\n",
      "b2:1.401186825525369e-07\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "\n",
    "(x_train,t_train), (x_test, t_Test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key] ))\n",
    "    print(key + ':' +str(diff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7.4 오차역전파법을 사용한 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09871666666666666 0.098\n",
      "0.9009833333333334 0.9033\n",
      "0.9191666666666667 0.9211\n",
      "0.9330166666666667 0.9313\n",
      "0.9430166666666666 0.9403\n",
      "0.9494 0.9463\n",
      "0.9564 0.9528\n",
      "0.9599 0.9568\n",
      "0.9628166666666667 0.96\n",
      "0.9658333333333333 0.9631\n",
      "0.9679166666666666 0.9638\n",
      "0.9703833333333334 0.9648\n",
      "0.9724666666666667 0.9661\n",
      "0.9740166666666666 0.965\n",
      "0.9756833333333333 0.968\n",
      "0.9769666666666666 0.9683\n",
      "0.9783333333333334 0.9682\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test,t_test) = load_mnist(normalize= True, one_hot_label = True)\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size/ batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -=  learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
