{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 자연어와 단어의 분산 표현\n",
    "\n",
    "## 자연어 처리란?\n",
    "\n",
    "우리의 말을 컴퓨터에게 이해시키기 위한 기술\n",
    "\n",
    "## Thesaurus\n",
    "\n",
    "thesauras란 기본적으로 유의어 사전으로, '뜻이 같은 단어(동의어)', '뜻이 비슷한 단어(유의어)'가 한 그룹으로 분류되어 있습니다.\n",
    "\n",
    "또한 자연어 처리에 이용되는 thesaurus에서는 단어 사이의 '상위와 하위' 혹은 '천제와 부분'등, 더 세세한 관계가지 정의해둔 경우가 있습니다.\n",
    "\n",
    "그 중 자연어 처리 분야에서 가장 유명한 thesaurus는 WordNet이다. \n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 2-2.png\" width=\"50%\" height=\"20%\" />    \n",
    "\n",
    "### Thesaurus의 문제점\n",
    "\n",
    "    * 시대 변화에 대응하기 어렵다.\n",
    "    \n",
    "    * 사람을 쓰는 비용이 크다.\n",
    "    \n",
    "    * 단어의 미묘한 차이를 표현할 수 없다.\n",
    "    \n",
    "## 통계 기반 기법\n",
    "\n",
    "대량의 텍스트 데이터인 Copus를 이용한다. \n",
    "\n",
    "통계 기반 기법의 목표는 문장을 쓰는 방법, 단어를 선택하는 방법, 단어의 의미 등 사람이 알고 있는 자연어에 대한 지식이 포함되어 있는 corpus를 \n",
    "\n",
    "이용하여 사람의 지식이 가득한 corpus에서 자동으로, 효율적으로 그 핵심을 추출하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이썬으로 corpus 전처리하기\n",
    "\n",
    "텍스트 데이터를 단어롤 분할하고 그 분할된 단어들을 단어 ID 목록으로 변환하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you say goodbye and i say hello .\n",
      "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "text = 'You say goodbye and I say hello.'\n",
    "text = text.lower()\n",
    "text = text.replace('.', ' .')\n",
    "print (text)\n",
    "\n",
    "words = text.split(' ')\n",
    "print (words)\n",
    "\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in words:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word\n",
    "        \n",
    "print (id_to_word)        \n",
    "print (word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n"
     ]
    }
   ],
   "source": [
    "corpus = [word_to_id[w] for w in words]\n",
    "corpus = np.array(corpus)\n",
    "print (corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "    \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어의 분산 표현\n",
    "\n",
    "단어의 분산 표현은 단어를 고정 길이의 밀집벡터(dense vector)로 표현하는 것을 말한다. 밀집 벡터라 함은 대부분의 원소가 0이 아닌 실수인 벡터를 말한다.\n",
    "\n",
    "### 분포 가설\n",
    "\n",
    "자연어에 관한 대부분의 연구는 '단어의 의미는 주변 단어에 의해 형성된다'라는 아이디어에 기반한다. 이를 __분포 가설__이라고 한다. \n",
    "\n",
    "단어 자체에는 의미가 없고, 그 단어가 사용된 '맥락'이 의미를 형성한다는 것이다. \n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 2-3.png\" width=\"50%\" height=\"20%\" />   \n",
    "\n",
    "### 동시발생 행렬\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import preprocess\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print (corpus)\n",
    "# [0 1 2 3 4 1 5 6]\n",
    "\n",
    "print (id_to_word)\n",
    "# {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"deep_learning_2_images/fig 2-6.png\" width=\"50%\" height=\"20%\" />   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vacab_size, window_size=1):\n",
    "    corpus_size=len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "            \n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_wor_id] += 1\n",
    "                \n",
    "            if right_idx < corpus_size:\n",
    "                right_word_idx = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx : 0 word_id : 0\n",
      "idx : 1 word_id : 1\n",
      "idx : 2 word_id : 2\n",
      "idx : 3 word_id : 3\n",
      "idx : 4 word_id : 4\n",
      "idx : 5 word_id : 1\n",
      "idx : 6 word_id : 5\n",
      "idx : 7 word_id : 6\n"
     ]
    }
   ],
   "source": [
    "co_matrix = np.zeros((3, 3), dtype=np.int32)\n",
    "    \n",
    "for idx, word_id in enumerate(corpus):\n",
    "    print ('idx :', idx, 'word_id :', word_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"deep_learning_2_images/fig 2-7.png\" width=\"50%\" height=\"20%\" />   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터 간 유사도\n",
    "\n",
    "벡터 사이의 유사도를 측정하는 방법은 다양하다. 대표적으로 벡터의 내적이나 유클리드 거리 등이 있다. \n",
    "\n",
    "단어 벡터의 유사도를 나타낼 때는 __코사인 유사도(cosine similarity)__를 자주 이용한다. \n",
    "\n",
    "두 벡터 $\\textbf {x} = (x_1, x_2, x_3, \\cdots, x_n)$ 과 $\\textbf {y} = (y_1, y_2, y_3, \\cdots, y_n)$ 이 있다면, 코사인 유사도는 다음 식으로 정의된다.\n",
    "\n",
    "$$ similarity\\textbf {(x, y)} = {\\textbf{x} \\cdot \\textbf{y} \\over \\|\\textbf{x}\\| \\|\\textbf{y}\\|} = {x_1y_1 + \\cdots + x_ny_n \\over \\sqrt{x_1^2+\\cdots+x_n^2} \\sqrt{y_1^2+\\cdots+y_n^2}}$$\n",
    "\n",
    "직관적으로 풀어보면 '두 벡터가 가리키는 방향이 얼마나 비슷한가?'이다. 두 벡터의 방향이 완전히 같다면 코사인 유사도가 1이 되며, 완전히 반대라면 -1이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1e-8을 더해주는 이유는 0으로 나누는 것을 방지하기 위함이다.\n",
    "# 이 정도 작은 값이면 일반적으로 부동소수점 계산 시 '반올림'되어 다른 값에 '흡수'됩니다. \n",
    "# 이 값이 벡터의 노름에 '흡수'되기 때문에 대부분의 경우 eps를 더한다고 해서 최종 계산 결과에는 영향을 주지 않는다.\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    nx = x / np.sqrt(np.sum(x**2) + eps) # x의 정규화\n",
    "    ny = y / np.sqrt(np.sum(y**2) + eps) # y의 정규화\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067691154799\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "from common.util import preprocess, create_co_matrix, cos_similarity\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "c0 = C[word_to_id['you']] # \"you\"의 단어 벡터\n",
    "c1 = C[word_to_id['i']] # \"i\"의 단어 벡터\n",
    "print (cos_similarity(c0, c1))\n",
    "# .7071067691154799"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n"
     ]
    }
   ],
   "source": [
    "print (corpus)\n",
    "print (id_to_word)\n",
    "print (word_to_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similarity(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    # 1. 검색어를 꺼낸다.\n",
    "    if query not in word_to_id:\n",
    "        print (\"'{0}'(을)를 찾을 수 없습니다.\".format(query))\n",
    "        return\n",
    "    \n",
    "    print ('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "    \n",
    "    # 2. 코사인 유사도 계산\n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vacab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "        \n",
    "    # 3. 코사인 유사도를 기준으로 내림차순으로 출력\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print (\"'{0}': '{1}'\".format(id_to_word[i], similarity[i]))\n",
    "        \n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0]\n",
      "[0 2 1]\n"
     ]
    }
   ],
   "source": [
    "# argsort() 설명 \n",
    "# 오름차순으로 정렬해서 인덱스를 반환한다.\n",
    "# 내림차순으로 정렬하고 싶으면 -를 곱하고 argsort()를 해준다.\n",
    "x = np.array([100, -20, 2])\n",
    "print (x.argsort())\n",
    "\n",
    "print ((-x).argsort())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " goodbye: 0.7071067691154799\n",
      " i: 0.7071067691154799\n",
      " hello: 0.7071067691154799\n",
      " say: 0.0\n",
      " and: 0.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import preprocess, create_co_matrix, most_similar\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "most_similar('you', word_to_id, id_to_word, C, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  통계 기반 기법 개선하기\n",
    "\n",
    "### 상호정보량\n",
    "\n",
    "동시발생 행렬의 원소는 두 단어가 동시에 발생한 횟수를 나타낸다. 그러나 이것은 좋은 특징이 아니다.\n",
    "\n",
    "그래서 __점별 상호정보량(Pointwise Mutual Information, PMI)__라는 척도를 사용하자.\n",
    "\n",
    "PMI는 확률변수 x와 y에 대해 다음 식으로 정의된다.\n",
    "\n",
    "$$ \\textbf{PMI}(x,y)  = \\log_2{P(x,y) \\over P(x)P(y)}$$\n",
    "\n",
    "동시발생 행렬을 사용하여 위의 식을 다시 나타내자. \n",
    "\n",
    "C는 동시발생 행렬, $C(x,y)$는 단어$x$와 단어$y$가 동시발생하는 횟수, $C(x)$와 $C(y)$는 각각 단어 $x$와 $y$의 등장 횟수, $N$은corpus에 포함된 단어 수\n",
    "\n",
    "$$ \\textbf{PMI}(x,y)  = \\log_2{P(x,y) \\over P(x)P(y)} = \\log_2{{C(x,y) \\over N} \\over {C(x) \\over N}{C(y) \\over N}} = \\log_2{C(x,y)\\cdot N \\over C(x)C(y)}$$\n",
    "\n",
    "그러나 두 단어의 동시발생 횟수가 0이면 $\\log_2{0} = -\\infty$가 된다.\n",
    "\n",
    "그래서 실제로는 __양의 상호정보량(Positive PMI)__를 사용한다.\n",
    "\n",
    "$$\\textbf{PPMI}(x,y) = max(0,\\textbf{PMI}(x,y))$$\n",
    "\n",
    "이 식에 따라 PMI가 음수일 때는 0으로 취급한다. \n",
    "\n",
    "__<질문>__\n",
    "\n",
    "근데 왜 로그 밑이 2일까??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmi(C, verbose=False, eps=1e-8):\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum()\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            # 로그값이 음의 무한대가 되는 것을 방지하기 위해 작은 값을 더했다.\n",
    "            pmi = np.log2(c[i, j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "            \n",
    "            if verbos:\n",
    "                \n",
    "                cnt += 1\n",
    "                if cnt % (total//100) == 0:\n",
    "                    print (\"'{0}'.1% 완료\".format(100*cnt/total))\n",
    "                    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시발생\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "--------------------------------------------------\n",
      "PPMI\n",
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import preprocess, create_co_matrix, cos_similarity, ppmi\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C)\n",
    "\n",
    "np.set_printoptions(precision=3) # 유효 자릿수를 세 자리로 표시\n",
    "print ('동시발생')\n",
    "print (C)\n",
    "print ('-'*50)\n",
    "print ('PPMI')\n",
    "print (W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 행렬을 보면 원소 대부분이 0인 것을 알 수있다. 벡터의 원소 대부분이 중요하지 않다는 뜻이다. 다르게 표현하면 각 원소의 '중요도'가 낮다는 뜻이다. 이런 벡터는 노이즈에 약하고 견고하지 못하다는 단점이 있다. \n",
    "\n",
    "이 문제를 해결하기 위해 __차원 감소__기법이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 차원 감소\n",
    "\n",
    "'중요한 정보'는 최대한 유지하면서 줄이는게 핵심이다. 직관적으로 데이터의 분포를 고려해 중요한 '축'을 찾는다. \n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 2-8.png\" width=\"50%\" height=\"20%\" />   \n",
    "\n",
    "__<NOTE>__\n",
    "    \n",
    "    차원 감소의 결과로 원래의 sparse vector는 원소 대부분이 0이 아닌 값으로 구성된 '밀집벡터'로 변환된다. 이 조밀한 벡터야말로 우리가 원하는 단어의 분산 표현이다. \n",
    "    \n",
    "차원 감소시키는 여러 방법 중, __특잇값분해(Singular Value Decomposition, SVD)__를 알아보자!    \n",
    "\n",
    "SVD는 임의의 행렬을 세 행렬의 곱으로 분해하며, 수식으로는 다음과 같다.\n",
    "\n",
    "$$ \\textbf{X}=\\textbf{USV}^T \\qquad{\\textbf{U}, \\textbf{V} : orthogonal\\, matrix, \\textbf{S} : diagonal\\, matrix}$$\n",
    "\n",
    "행렬 __S__에서 특잇값이 작다면 중요도가 낮다는 뜻이므로 __S__, __U__, __V__의 일부분만 가져와 원래 행렬 __X__에 근사시킬 수 있다.\n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 2-9.png\" width=\"50%\" height=\"20%\" />  \n",
    "\n",
    "에서\n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 2-10.png\" width=\"50%\" height=\"20%\" />   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD에 의한 차원 감소\n",
    "\n",
    "동시발생 행렬을 만들어 PPMI행렬로 변환한 다음 SVD를 적용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from common.util import preprocess, create_co_matrix, ppmi\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(id_to_word)\n",
    "C = create_co_matrix(corpus, vocab_size, window_size=1)\n",
    "W = ppmi(C)\n",
    "\n",
    "# SVD\n",
    "U, S, V = np.linalg.svd(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n",
      "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      "[ 3.409e-01 -1.110e-16 -3.886e-16 -1.205e-01 -9.323e-01  0.000e+00\n",
      " -1.800e-16]\n",
      "[ 3.409e-01 -1.110e-16]\n"
     ]
    }
   ],
   "source": [
    "print (C[0])\n",
    "print (W[0])\n",
    "print (U[0]) \n",
    "\n",
    "# 밀집벡터의 차원을 2차원으로 감소시키는 방법\n",
    "print (U[0,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 시각화\n",
    "# 책에 의하면 goodbye와 hello, you와 i가 가까이 있다는 왜 그러지...?\n",
    "for word, word_to_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_to_id, 0], U[word_to_id, 1]))\n",
    "    \n",
    "plt.scatter(U[:,0], U[:,1], alpha=.5)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PTB 데이터셋\n",
    "\n",
    "PTB corpus는 주로 주어진 기법의 성능을 측정하는 벤치마크로 자주 이용된다.\n",
    "\n",
    "PTB corpus에서는 한 문장이 하나의 줄로 저장되어 있다. \n",
    "\n",
    "\n",
    "이 책에서는 각 문장을 연결한 '하나의 큰 시계열 데이터'로 취급한다. 이때 각 문장 끝에 <eos>라는 특수문자를 삽입한다?? 무슨 말이지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ptb.train.txt ... \n",
      "Done\n",
      "말뭉치 크기 :  929589\n",
      "corpus[:30] :  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "id_to_word[0] :  aer\n",
      "id_to_word[1] :  banknote\n",
      "id_to_word[2] :  berlitz\n",
      "\n",
      "word_to_id['car'] :  3856\n",
      "word_to_id['happy'] :  4428\n",
      "word_to_id['lexus'] :  7426\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset import ptb\n",
    "\n",
    "# train용으로 데이터를 사용하겠다는 뜻\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "\n",
    "print ('말뭉치 크기 : ', len(corpus))\n",
    "print ('corpus[:30] : ', corpus[:30])\n",
    "print ()\n",
    "print ('id_to_word[0] : ', id_to_word[0])\n",
    "print ('id_to_word[1] : ', id_to_word[1])\n",
    "print ('id_to_word[2] : ', id_to_word[2])\n",
    "print ()\n",
    "print (\"word_to_id['car'] : \", word_to_id['car'])\n",
    "print (\"word_to_id['happy'] : \", word_to_id['happy'])\n",
    "print (\"word_to_id['lexus'] : \", word_to_id['lexus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PTB 데이터셋 평가\n",
    "\n",
    "고속 SVD를 이용하여 PTB 데이터셋에 통계 기반 기법을 적용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시발생 수 계산...\n",
      "PPMI 계산...\n",
      "1.0% 완료\n",
      "2.0% 완료\n",
      "3.0% 완료\n",
      "4.0% 완료\n",
      "5.0% 완료\n",
      "6.0% 완료\n",
      "7.0% 완료\n",
      "8.0% 완료\n",
      "9.0% 완료\n",
      "10.0% 완료\n",
      "11.0% 완료\n",
      "12.0% 완료\n",
      "13.0% 완료\n",
      "14.0% 완료\n",
      "15.0% 완료\n",
      "16.0% 완료\n",
      "17.0% 완료\n",
      "18.0% 완료\n",
      "19.0% 완료\n",
      "20.0% 완료\n",
      "21.0% 완료\n",
      "22.0% 완료\n",
      "23.0% 완료\n",
      "24.0% 완료\n",
      "25.0% 완료\n",
      "26.0% 완료\n",
      "27.0% 완료\n",
      "28.0% 완료\n",
      "29.0% 완료\n",
      "30.0% 완료\n",
      "31.0% 완료\n",
      "32.0% 완료\n",
      "33.0% 완료\n",
      "34.0% 완료\n",
      "35.0% 완료\n",
      "36.0% 완료\n",
      "37.0% 완료\n",
      "38.0% 완료\n",
      "39.0% 완료\n",
      "40.0% 완료\n",
      "41.0% 완료\n",
      "42.0% 완료\n",
      "43.0% 완료\n",
      "44.0% 완료\n",
      "45.0% 완료\n",
      "46.0% 완료\n",
      "47.0% 완료\n",
      "48.0% 완료\n",
      "49.0% 완료\n",
      "50.0% 완료\n",
      "51.0% 완료\n",
      "52.0% 완료\n",
      "53.0% 완료\n",
      "54.0% 완료\n",
      "55.0% 완료\n",
      "56.0% 완료\n",
      "57.0% 완료\n",
      "58.0% 완료\n",
      "59.0% 완료\n",
      "60.0% 완료\n",
      "61.0% 완료\n",
      "62.0% 완료\n",
      "63.0% 완료\n",
      "64.0% 완료\n",
      "65.0% 완료\n",
      "66.0% 완료\n",
      "67.0% 완료\n",
      "68.0% 완료\n",
      "69.0% 완료\n",
      "70.0% 완료\n",
      "71.0% 완료\n",
      "72.0% 완료\n",
      "73.0% 완료\n",
      "74.0% 완료\n",
      "75.0% 완료\n",
      "76.0% 완료\n",
      "77.0% 완료\n",
      "78.0% 완료\n",
      "79.0% 완료\n",
      "80.0% 완료\n",
      "81.0% 완료\n",
      "82.0% 완료\n",
      "83.0% 완료\n",
      "84.0% 완료\n",
      "85.0% 완료\n",
      "86.0% 완료\n",
      "87.0% 완료\n",
      "88.0% 완료\n",
      "89.0% 완료\n",
      "90.0% 완료\n",
      "91.0% 완료\n",
      "92.0% 완료\n",
      "93.0% 완료\n",
      "94.0% 완료\n",
      "95.0% 완료\n",
      "96.0% 완료\n",
      "97.0% 완료\n",
      "98.0% 완료\n",
      "99.0% 완료\n",
      "100.0% 완료\n",
      "SVD 계산...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 100 is out of bounds for axis 1 with size 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-250b9aee58a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mword_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordvec_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mquerys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'you'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'car'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'toyota'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 100 is out of bounds for axis 1 with size 100"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import most_similar, create_co_matrix, ppmi\n",
    "from dataset import ptb\n",
    "\n",
    "window_size = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "print ('동시발생 수 계산...')\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "print ('PPMI 계산...')\n",
    "W = ppmi(C, verbose=True)\n",
    "\n",
    "print ('SVD 계산...')\n",
    "try:\n",
    "    #truncated SVD 빠름\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,\n",
    "                            random_state=None)\n",
    "    \n",
    "except ImportError:\n",
    "    # SVD 느림\n",
    "    U, S, V = np.linalg.svd(W)\n",
    "    \n",
    "word_vecs = U[:, wordvec_size]    \n",
    "\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vec, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이번 장에서 배운 내용\n",
    "\n",
    "    * WordNet 등의 thesaurus를 이용하며 유의어를 얻거나 단어 사이의 유사도를 측정하는등 유용한 작업을 할 수 있다.\n",
    "    \n",
    "    * thesaurus 기반 기법은 thesaurus를 작성하는 데 엄청난 인적 자원이 든다거나 새로운 단어에 대응하기 어렵다는 문제가 있다.\n",
    "    \n",
    "    * 현재는 corpus(말뭉치)를 이용해 단어를 벡터화하는 방식이 주로 쓰인다.\n",
    "    \n",
    "    * 최근의 단어 벡터화 기법들은 대부분 '단어의 의미는 주변 단어에 의해 형성된다'는 분포 가설에 기초한다.\n",
    "    \n",
    "    * 통계 기반 기법은 말뭉치 안의 각 단어에 대해서 그 단어의 주변 단어의 빈도를 집계한다(동시발생 행렬).\n",
    "    \n",
    "    * 동시발생 행렬을 PPMI 행렬로 변환하고 다시 차원을 감소시킴으로써, 거대한 '희소벡터'를 작은 '밀집벡터'로 변호나할 수 있다.\n",
    "    \n",
    "    * 단어의 벡터 공간에서는 의미가 가까운 단어는 그 거리도 가까울 것으로 기대된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "\n",
    "앞 장에서는 '통계 기반 기법'으로 단어의 분산 표현을 얻었는데, 이번 장에서는 더 강력한 기법인 '추론 기반 기법'을 배워보자!\n",
    "\n",
    "word2vec을 구현하여 추론 기반 기법을 배워보자!\n",
    "\n",
    "## 추론 기반 기법과 신경망\n",
    "\n",
    "단어를 벡터로 표현하는 방법 중 크게 2가지가 있다. 통계 기반과 추론 기반이다.\n",
    "\n",
    "이번 절에선 통계 기반의 문제점을 지적하고 추론 기반을 알아보자.\n",
    "\n",
    "후에 word2vec의 전처리를 위해 신경망으로 '단어'를 처리하는 예를 보자.\n",
    "\n",
    "### 통계 기반 기법의 문제점\n",
    "\n",
    "SVD의 계산 복잡도는 $O(n^3)$이므로 대규모 corpus를 다룰 때 상당한 병목현상을 일으킨다.  \n",
    "\n",
    "### 추론 기반 기법 개요\n",
    "\n",
    "맥락이 주어졌을 때 어떤 단어가 들어갈지를 추측하는 것이다. 말 그대로 '추론'하는 것이다.\n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 3-2.png\" width=\"50%\" height=\"20%\"> \n",
    "\n",
    "어떤 추론 모델에 맥락이 이력되면 각 단어의 출현 확률을 출력한다.\n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 3-3.png\" width=\"50%\" height=\"20%\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망에서의 단어 처리\n",
    "\n",
    "기본적으로 '고정 길이 벡터'로 표현을 해야 한다. 그래서 보통 원핫벡터로 변환시킨다.\n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 3-4.png\" width=\"50%\" height=\"20%\" /> \n",
    "\n",
    "다음은 원핫표현으로 된 단어 하나를 완전연결계층을 통해 변환하는 과정이다\n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 3-7.png\" width=\"50%\" height=\"20%\" /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "c = np.array([1,0,0,0,0,0,0])\n",
    "W = np.random.randn(7,3)\n",
    "h = np.matmul(c, W)\n",
    "print (h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 실제로 원핫벡터와 가중치행렬의 연산을 들여다 보면 가중치행렬에서 행벡터 하나를 뽑아낸 것과 같다.\n",
    "\n",
    "그 과정을 다음 장에서 word2vec의 속도 개선하는 방법으로 사용하겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단순한 word2vec\n",
    "\n",
    "이번 절에서 사용할 신경망모델은 word2vec에서 제안하는 CBOW(Continuous bag of words)모델이다.\n",
    "\n",
    "### CBOW 모델의 추론 처리\n",
    "\n",
    "CBOW모델은 맥ㄺ으로부터 타깃을 추측하는 용도의 신경망이다.\n",
    "\n",
    "그래서 CBOW 모델이 가능한 한 정확하게 추론하도록 훈련시켜서 단어으이 분산 표현을 얻어내야 한다.\n",
    "\n",
    "다음은 CBOW 모델의 신경망 구조이다.\n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 3-9.png\" width=\"50%\" height=\"20%\" /> \n",
    "\n",
    "여기서 은닉층에 주목하자.\n",
    "\n",
    "입력층이 2개가 있는 것을 볼 수 있다. 어떤 값을 은닉층의 값으로 써야 할 지 생각을 해줘야 한다.\n",
    "\n",
    "이때 각 입력층의 출력값을 평균내서 은닉층의 값으로 사용한다.\n",
    "\n",
    "그리고 입력층의 가중치행렬 $\\textbf{W}_{in}$이 단어의 분산표현이다.\n",
    "\n",
    "다음의 그림이 계층관점에서 본 CBOW모델의 구조이다.\n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 3-11.png\" width=\"50%\" height=\"20%\" /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.055  1.04  -0.731  0.302  0.085  0.638  1.138]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import MatMul\n",
    "\n",
    "# 샘플 맥락 데이터\n",
    "c0 = np.array([1,0,0,0,0,0,0])\n",
    "c1 = np.array([0,0,1,0,0,0,0])\n",
    "\n",
    "# 가중치 초기화\n",
    "W_in = np.random.randn(7,3)\n",
    "W_out = np.random.randn(3,7)\n",
    "\n",
    "# 계층 생성\n",
    "in_layer0 = MatMul(W_in)\n",
    "in_layer1 = MatMul(W_in)\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "# 순전파\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer1.forward(c1)\n",
    "h = .5 * (h0+h1)\n",
    "s = out_layer.forward(h)\n",
    "\n",
    "print (s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW 모델의 학습\n",
    "\n",
    "출력층에서 각 단어의 점수를 출력했다. 이 값에 소프트맥스 함수를 적용하면 확률값을 얻을 수 있다.\n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 3-12.png\" width=\"50%\" height=\"20%\" /> \n",
    "\n",
    "이 신경망을 학습하기 위해서 소프트맥스 함수와 교차엔트로피 오차를 이용한다.\n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 3-14.png\" width=\"50%\" height=\"20%\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec의 가중치와 분산 표현\n",
    "\n",
    "word2vec에는 두 가지의 가중치가 있다. 완전연결계층의 가중치$\\textbf(W)_{in}$과 출력 측 완전연결계층의 가중치$\\textbf(W)_{out}$이다.\n",
    "\n",
    "입력 측 가중치의 각 행이 각 단어의 분산 표현이고 출력 측 가중치에도 단어의 의미가 인코딩된 벡터각 저장되고 있다고 생각할 수 있다.\n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 3-15.png\" width=\"50%\" height=\"20%\" /> \n",
    "\n",
    "그럼 어떤 가중치를 단어의 분산 표현으로 사용할까?\n",
    "\n",
    "    1. 입력 측의 가중치만 이용한다.\n",
    "    \n",
    "    2. 출력 측의 가중치만 이용한다.\n",
    "    \n",
    "    3. 양쪽 가중치 모두 이용한다.\n",
    "    \n",
    "word2vec(특히 skip-gram 모델)에서는 1이 대중적이다.\n",
    "\n",
    "* word2vec과 유사한 GloVe에서는 두 가중치를 더한 값을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터 준비\n",
    "\n",
    "예시로 'You say goodby ans I say hello.'라는 문장을 사용한다.\n",
    "\n",
    "### 맥락과 타깃\n",
    "\n",
    "먼저 preprocess()함수를 이용하여 corpus를 단어 ID로 변환해주자.\n",
    "\n",
    "다음 사진과 같이 만들어주는 create_contexts_target()함수를 만들어주자.\n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 3-16.png\" width=\"50%\" height=\"20%\" /> \n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 3-17.png\" width=\"50%\" height=\"20%\" /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodby', 3: 'ans', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import preprocess\n",
    "\n",
    "text = 'You say goodby ans I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print (corpus)\n",
    "print (id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contexts_target(corpus, window_size=1):\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "    \n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])    \n",
    "        contexts.append(cs)   \n",
    "    \n",
    "    return np.array(contexts), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "[1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "print (contexts)\n",
    "print (target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원핫 표현으로 변환\n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 3-18.png\" width=\"50%\" height=\"20%\" /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''원핫 표현으로 변환\n",
    "\n",
    "    :param corpus: 단어 ID 목록(1차원 또는 2차원 넘파이 배열)\n",
    "    :param vocab_size: 어휘 수\n",
    "    :return: 원핫 표현(2차원 또는 3차원 넘파이 배열)\n",
    "    '''\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지의 데이터 준비과정을 정리해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "text = 'You say goodby ans I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW 모델 구현\n",
    "\n",
    "구현해야 할 모델은 다음과 같다.\n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 3-19.png\" width=\"50%\" height=\"20%\" /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import MatMul, SoftmaxWithLoss\n",
    "\n",
    "\n",
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
    "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
    "        h = (h0 + h1) * 0.5\n",
    "        score = self.out_layer.forward(h)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        self.in_layer1.backward(da)\n",
    "        self.in_layer0.backward(da)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순전파\n",
    "def forward(self, contexts, target):\n",
    "    h0 = self.in_layer0.forward(contexts[:,0])\n",
    "    h1 = self.in_layer1.forward(contexts[:,1])\n",
    "    h = (h0 + h1) * .5\n",
    "    score = self.out_layer.forward(h)\n",
    "    loss = self.loss_layer.forward(score, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역전파 계산그래프의 설명이다.\n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 3-20.png\" width=\"50%\" height=\"20%\" /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역전파\n",
    "\n",
    "def backward(self, dout=1):\n",
    "    ds = self.loss_layer.backward(dout)\n",
    "    da = self.out_layer.backward(ds)\n",
    "    da *= .5\n",
    "    self.in_layer1.backward(da)\n",
    "    self.in_layer0.backward(da)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 2 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 3 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 4 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 5 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 6 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 7 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 8 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 9 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 10 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 11 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 12 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 13 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 14 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 15 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 16 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 17 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 18 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 19 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 20 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 21 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 22 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 23 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 24 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 25 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 26 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 27 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 28 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 29 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 30 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 31 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 32 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 33 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 34 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 35 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 36 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 37 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 38 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 39 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 40 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 41 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 42 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 43 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 44 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 45 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 46 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 47 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 48 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 49 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 50 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 51 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 52 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 53 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
      "| 에폭 54 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
      "| 에폭 55 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
      "| 에폭 56 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
      "| 에폭 57 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
      "| 에폭 58 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
      "| 에폭 59 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
      "| 에폭 60 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
      "| 에폭 61 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
      "| 에폭 62 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
      "| 에폭 63 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
      "| 에폭 64 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
      "| 에폭 65 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
      "| 에폭 66 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
      "| 에폭 67 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
      "| 에폭 68 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
      "| 에폭 69 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
      "| 에폭 70 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
      "| 에폭 71 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
      "| 에폭 72 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
      "| 에폭 73 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
      "| 에폭 74 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
      "| 에폭 75 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
      "| 에폭 76 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
      "| 에폭 77 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
      "| 에폭 78 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
      "| 에폭 79 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
      "| 에폭 80 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
      "| 에폭 81 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
      "| 에폭 82 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
      "| 에폭 83 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
      "| 에폭 84 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
      "| 에폭 85 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
      "| 에폭 86 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
      "| 에폭 87 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
      "| 에폭 88 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
      "| 에폭 89 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
      "| 에폭 90 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
      "| 에폭 91 |  반복 1 / 2 | 시간 0[s] | 손실 1.75\n",
      "| 에폭 92 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
      "| 에폭 93 |  반복 1 / 2 | 시간 0[s] | 손실 1.75\n",
      "| 에폭 94 |  반복 1 / 2 | 시간 0[s] | 손실 1.76\n",
      "| 에폭 95 |  반복 1 / 2 | 시간 0[s] | 손실 1.75\n",
      "| 에폭 96 |  반복 1 / 2 | 시간 0[s] | 손실 1.75\n",
      "| 에폭 97 |  반복 1 / 2 | 시간 0[s] | 손실 1.72\n",
      "| 에폭 98 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
      "| 에폭 99 |  반복 1 / 2 | 시간 0[s] | 손실 1.74\n",
      "| 에폭 100 |  반복 1 / 2 | 시간 0[s] | 손실 1.73\n",
      "| 에폭 101 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
      "| 에폭 102 |  반복 1 / 2 | 시간 0[s] | 손실 1.73\n",
      "| 에폭 103 |  반복 1 / 2 | 시간 0[s] | 손실 1.73\n",
      "| 에폭 104 |  반복 1 / 2 | 시간 0[s] | 손실 1.70\n",
      "| 에폭 105 |  반복 1 / 2 | 시간 0[s] | 손실 1.72\n",
      "| 에폭 106 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
      "| 에폭 107 |  반복 1 / 2 | 시간 0[s] | 손실 1.68\n",
      "| 에폭 108 |  반복 1 / 2 | 시간 0[s] | 손실 1.70\n",
      "| 에폭 109 |  반복 1 / 2 | 시간 0[s] | 손실 1.73\n",
      "| 에폭 110 |  반복 1 / 2 | 시간 0[s] | 손실 1.66\n",
      "| 에폭 111 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
      "| 에폭 112 |  반복 1 / 2 | 시간 0[s] | 손실 1.68\n",
      "| 에폭 113 |  반복 1 / 2 | 시간 0[s] | 손실 1.64\n",
      "| 에폭 114 |  반복 1 / 2 | 시간 0[s] | 손실 1.67\n",
      "| 에폭 115 |  반복 1 / 2 | 시간 0[s] | 손실 1.68\n",
      "| 에폭 116 |  반복 1 / 2 | 시간 0[s] | 손실 1.68\n",
      "| 에폭 117 |  반복 1 / 2 | 시간 0[s] | 손실 1.63\n",
      "| 에폭 118 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
      "| 에폭 119 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
      "| 에폭 120 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
      "| 에폭 121 |  반복 1 / 2 | 시간 0[s] | 손실 1.63\n",
      "| 에폭 122 |  반복 1 / 2 | 시간 0[s] | 손실 1.63\n",
      "| 에폭 123 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
      "| 에폭 124 |  반복 1 / 2 | 시간 0[s] | 손실 1.63\n",
      "| 에폭 125 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
      "| 에폭 126 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
      "| 에폭 127 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
      "| 에폭 128 |  반복 1 / 2 | 시간 0[s] | 손실 1.56\n",
      "| 에폭 129 |  반복 1 / 2 | 시간 0[s] | 손실 1.66\n",
      "| 에폭 130 |  반복 1 / 2 | 시간 0[s] | 손실 1.59\n",
      "| 에폭 131 |  반복 1 / 2 | 시간 0[s] | 손실 1.55\n",
      "| 에폭 132 |  반복 1 / 2 | 시간 0[s] | 손실 1.57\n",
      "| 에폭 133 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
      "| 에폭 134 |  반복 1 / 2 | 시간 0[s] | 손실 1.56\n",
      "| 에폭 135 |  반복 1 / 2 | 시간 0[s] | 손실 1.54\n",
      "| 에폭 136 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
      "| 에폭 137 |  반복 1 / 2 | 시간 0[s] | 손실 1.53\n",
      "| 에폭 138 |  반복 1 / 2 | 시간 0[s] | 손실 1.57\n",
      "| 에폭 139 |  반복 1 / 2 | 시간 0[s] | 손실 1.59\n",
      "| 에폭 140 |  반복 1 / 2 | 시간 0[s] | 손실 1.50\n",
      "| 에폭 141 |  반복 1 / 2 | 시간 0[s] | 손실 1.54\n",
      "| 에폭 142 |  반복 1 / 2 | 시간 0[s] | 손실 1.58\n",
      "| 에폭 143 |  반복 1 / 2 | 시간 0[s] | 손실 1.50\n",
      "| 에폭 144 |  반복 1 / 2 | 시간 0[s] | 손실 1.50\n",
      "| 에폭 145 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
      "| 에폭 146 |  반복 1 / 2 | 시간 0[s] | 손실 1.54\n",
      "| 에폭 147 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
      "| 에폭 148 |  반복 1 / 2 | 시간 0[s] | 손실 1.52\n",
      "| 에폭 149 |  반복 1 / 2 | 시간 0[s] | 손실 1.50\n",
      "| 에폭 150 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
      "| 에폭 151 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
      "| 에폭 152 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
      "| 에폭 153 |  반복 1 / 2 | 시간 0[s] | 손실 1.49\n",
      "| 에폭 154 |  반복 1 / 2 | 시간 0[s] | 손실 1.50\n",
      "| 에폭 155 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
      "| 에폭 156 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
      "| 에폭 157 |  반복 1 / 2 | 시간 0[s] | 손실 1.55\n",
      "| 에폭 158 |  반복 1 / 2 | 시간 0[s] | 손실 1.43\n",
      "| 에폭 159 |  반복 1 / 2 | 시간 0[s] | 손실 1.43\n",
      "| 에폭 160 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
      "| 에폭 161 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
      "| 에폭 162 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
      "| 에폭 163 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
      "| 에폭 164 |  반복 1 / 2 | 시간 0[s] | 손실 1.38\n",
      "| 에폭 165 |  반복 1 / 2 | 시간 0[s] | 손실 1.41\n",
      "| 에폭 166 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
      "| 에폭 167 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
      "| 에폭 168 |  반복 1 / 2 | 시간 0[s] | 손실 1.43\n",
      "| 에폭 169 |  반복 1 / 2 | 시간 0[s] | 손실 1.44\n",
      "| 에폭 170 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
      "| 에폭 171 |  반복 1 / 2 | 시간 0[s] | 손실 1.49\n",
      "| 에폭 172 |  반복 1 / 2 | 시간 0[s] | 손실 1.33\n",
      "| 에폭 173 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
      "| 에폭 174 |  반복 1 / 2 | 시간 0[s] | 손실 1.40\n",
      "| 에폭 175 |  반복 1 / 2 | 시간 0[s] | 손실 1.43\n",
      "| 에폭 176 |  반복 1 / 2 | 시간 0[s] | 손실 1.33\n",
      "| 에폭 177 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
      "| 에폭 178 |  반복 1 / 2 | 시간 0[s] | 손실 1.39\n",
      "| 에폭 179 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
      "| 에폭 180 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
      "| 에폭 181 |  반복 1 / 2 | 시간 0[s] | 손실 1.33\n",
      "| 에폭 182 |  반복 1 / 2 | 시간 0[s] | 손실 1.34\n",
      "| 에폭 183 |  반복 1 / 2 | 시간 0[s] | 손실 1.28\n",
      "| 에폭 184 |  반복 1 / 2 | 시간 0[s] | 손실 1.43\n",
      "| 에폭 185 |  반복 1 / 2 | 시간 0[s] | 손실 1.33\n",
      "| 에폭 186 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
      "| 에폭 187 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
      "| 에폭 188 |  반복 1 / 2 | 시간 0[s] | 손실 1.33\n",
      "| 에폭 189 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
      "| 에폭 190 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
      "| 에폭 191 |  반복 1 / 2 | 시간 0[s] | 손실 1.34\n",
      "| 에폭 192 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
      "| 에폭 193 |  반복 1 / 2 | 시간 0[s] | 손실 1.28\n",
      "| 에폭 194 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
      "| 에폭 195 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
      "| 에폭 196 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
      "| 에폭 197 |  반복 1 / 2 | 시간 0[s] | 손실 1.32\n",
      "| 에폭 198 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
      "| 에폭 199 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
      "| 에폭 200 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
      "| 에폭 201 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
      "| 에폭 202 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
      "| 에폭 203 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
      "| 에폭 204 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
      "| 에폭 205 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
      "| 에폭 206 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
      "| 에폭 207 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
      "| 에폭 208 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 209 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
      "| 에폭 210 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
      "| 에폭 211 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
      "| 에폭 212 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 213 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 214 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
      "| 에폭 215 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
      "| 에폭 216 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
      "| 에폭 217 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
      "| 에폭 218 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
      "| 에폭 219 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
      "| 에폭 220 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
      "| 에폭 221 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 222 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 223 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
      "| 에폭 224 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
      "| 에폭 225 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 226 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
      "| 에폭 227 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 228 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 229 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 230 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 231 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
      "| 에폭 232 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
      "| 에폭 233 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
      "| 에폭 234 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
      "| 에폭 235 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
      "| 에폭 236 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 237 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
      "| 에폭 238 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
      "| 에폭 239 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 240 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
      "| 에폭 241 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 242 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
      "| 에폭 243 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 244 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 245 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
      "| 에폭 246 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 247 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
      "| 에폭 248 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
      "| 에폭 249 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 250 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
      "| 에폭 251 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 252 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 253 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 254 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
      "| 에폭 255 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 256 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 257 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 258 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
      "| 에폭 259 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 260 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
      "| 에폭 261 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 262 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 263 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 264 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 265 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 266 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 267 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 268 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 269 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 270 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 271 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 272 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 273 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
      "| 에폭 274 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 275 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 276 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 277 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 278 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 279 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 280 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 281 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 282 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
      "| 에폭 283 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 284 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
      "| 에폭 285 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 286 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 287 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 288 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
      "| 에폭 289 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 290 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
      "| 에폭 291 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 292 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 293 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 294 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 295 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 296 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
      "| 에폭 297 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 298 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 299 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 300 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 301 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 302 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
      "| 에폭 303 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 304 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 305 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 306 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 307 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 308 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 309 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 310 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 311 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 312 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 313 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 314 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 315 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 316 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 317 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 318 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 319 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 320 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 321 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 322 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 323 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 324 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 325 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 326 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 327 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 328 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 329 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 330 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 331 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 332 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 333 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 334 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 335 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 336 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 337 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 338 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 339 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 340 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 341 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 342 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 343 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 344 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 345 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 346 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 347 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 348 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 349 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 350 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 351 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 352 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 353 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 354 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 355 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 356 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 357 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 358 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 359 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 360 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 361 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 362 |  반복 1 / 2 | 시간 1[s] | 손실 0.82\n",
      "| 에폭 363 |  반복 1 / 2 | 시간 1[s] | 손실 0.82\n",
      "| 에폭 364 |  반복 1 / 2 | 시간 1[s] | 손실 0.85\n",
      "| 에폭 365 |  반복 1 / 2 | 시간 1[s] | 손실 0.87\n",
      "| 에폭 366 |  반복 1 / 2 | 시간 1[s] | 손실 0.75\n",
      "| 에폭 367 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
      "| 에폭 368 |  반복 1 / 2 | 시간 1[s] | 손실 0.73\n",
      "| 에폭 369 |  반복 1 / 2 | 시간 1[s] | 손실 0.90\n",
      "| 에폭 370 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 371 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
      "| 에폭 372 |  반복 1 / 2 | 시간 1[s] | 손실 0.90\n",
      "| 에폭 373 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
      "| 에폭 374 |  반복 1 / 2 | 시간 1[s] | 손실 0.89\n",
      "| 에폭 375 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
      "| 에폭 376 |  반복 1 / 2 | 시간 1[s] | 손실 0.79\n",
      "| 에폭 377 |  반복 1 / 2 | 시간 1[s] | 손실 0.86\n",
      "| 에폭 378 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 379 |  반복 1 / 2 | 시간 1[s] | 손실 0.79\n",
      "| 에폭 380 |  반복 1 / 2 | 시간 1[s] | 손실 0.88\n",
      "| 에폭 381 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 382 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
      "| 에폭 383 |  반복 1 / 2 | 시간 1[s] | 손실 0.89\n",
      "| 에폭 384 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 385 |  반복 1 / 2 | 시간 1[s] | 손실 0.77\n",
      "| 에폭 386 |  반복 1 / 2 | 시간 1[s] | 손실 0.85\n",
      "| 에폭 387 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 388 |  반복 1 / 2 | 시간 1[s] | 손실 0.88\n",
      "| 에폭 389 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 390 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 391 |  반복 1 / 2 | 시간 1[s] | 손실 0.87\n",
      "| 에폭 392 |  반복 1 / 2 | 시간 1[s] | 손실 0.87\n",
      "| 에폭 393 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 394 |  반복 1 / 2 | 시간 1[s] | 손실 0.86\n",
      "| 에폭 395 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
      "| 에폭 396 |  반복 1 / 2 | 시간 1[s] | 손실 0.77\n",
      "| 에폭 397 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 398 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
      "| 에폭 399 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
      "| 에폭 400 |  반복 1 / 2 | 시간 1[s] | 손실 0.95\n",
      "| 에폭 401 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 402 |  반복 1 / 2 | 시간 1[s] | 손실 0.85\n",
      "| 에폭 403 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 404 |  반복 1 / 2 | 시간 1[s] | 손실 0.85\n",
      "| 에폭 405 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 406 |  반복 1 / 2 | 시간 1[s] | 손실 0.85\n",
      "| 에폭 407 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
      "| 에폭 408 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
      "| 에폭 409 |  반복 1 / 2 | 시간 1[s] | 손실 0.79\n",
      "| 에폭 410 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
      "| 에폭 411 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
      "| 에폭 412 |  반복 1 / 2 | 시간 1[s] | 손실 0.89\n",
      "| 에폭 413 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
      "| 에폭 414 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 415 |  반복 1 / 2 | 시간 1[s] | 손실 0.79\n",
      "| 에폭 416 |  반복 1 / 2 | 시간 1[s] | 손실 0.78\n",
      "| 에폭 417 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 418 |  반복 1 / 2 | 시간 1[s] | 손실 0.79\n",
      "| 에폭 419 |  반복 1 / 2 | 시간 1[s] | 손실 0.73\n",
      "| 에폭 420 |  반복 1 / 2 | 시간 1[s] | 손실 0.73\n",
      "| 에폭 421 |  반복 1 / 2 | 시간 1[s] | 손실 0.77\n",
      "| 에폭 422 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 423 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
      "| 에폭 424 |  반복 1 / 2 | 시간 1[s] | 손실 0.92\n",
      "| 에폭 425 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 426 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 427 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
      "| 에폭 428 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 429 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 430 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 431 |  반복 1 / 2 | 시간 1[s] | 손실 0.81\n",
      "| 에폭 432 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 433 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
      "| 에폭 434 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 435 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 436 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
      "| 에폭 437 |  반복 1 / 2 | 시간 1[s] | 손실 0.75\n",
      "| 에폭 438 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 439 |  반복 1 / 2 | 시간 1[s] | 손실 0.79\n",
      "| 에폭 440 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 441 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 442 |  반복 1 / 2 | 시간 1[s] | 손실 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 443 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 444 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
      "| 에폭 445 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 446 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 447 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 448 |  반복 1 / 2 | 시간 1[s] | 손실 0.88\n",
      "| 에폭 449 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 450 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 451 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 452 |  반복 1 / 2 | 시간 1[s] | 손실 0.82\n",
      "| 에폭 453 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 454 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 455 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
      "| 에폭 456 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 457 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 458 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 459 |  반복 1 / 2 | 시간 1[s] | 손실 0.73\n",
      "| 에폭 460 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 461 |  반복 1 / 2 | 시간 1[s] | 손실 0.77\n",
      "| 에폭 462 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 463 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 464 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
      "| 에폭 465 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 466 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 467 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 468 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 469 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 470 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 471 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
      "| 에폭 472 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
      "| 에폭 473 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 474 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 475 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 476 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
      "| 에폭 477 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 478 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
      "| 에폭 479 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 480 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 481 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 482 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 483 |  반복 1 / 2 | 시간 1[s] | 손실 0.79\n",
      "| 에폭 484 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 485 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 486 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
      "| 에폭 487 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 488 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
      "| 에폭 489 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 490 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 491 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 492 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 493 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 494 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 495 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 496 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 497 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 498 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 499 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 500 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 501 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 502 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 503 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 504 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 505 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 506 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 507 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
      "| 에폭 508 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 509 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 510 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 511 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 512 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 513 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 514 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 515 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 516 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 517 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 518 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 519 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 520 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 521 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 522 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 523 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 524 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 525 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 526 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 527 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 528 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 529 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 530 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 531 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 532 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 533 |  반복 1 / 2 | 시간 1[s] | 손실 0.73\n",
      "| 에폭 534 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 535 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 536 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 537 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 538 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 539 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 540 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
      "| 에폭 541 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 542 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
      "| 에폭 543 |  반복 1 / 2 | 시간 1[s] | 손실 0.40\n",
      "| 에폭 544 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
      "| 에폭 545 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 546 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
      "| 에폭 547 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 548 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 549 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 550 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 551 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 552 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 553 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 554 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 555 |  반복 1 / 2 | 시간 1[s] | 손실 0.32\n",
      "| 에폭 556 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 557 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 558 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 559 |  반복 1 / 2 | 시간 1[s] | 손실 0.35\n",
      "| 에폭 560 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
      "| 에폭 561 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 562 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 563 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 564 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 565 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 566 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
      "| 에폭 567 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
      "| 에폭 568 |  반복 1 / 2 | 시간 1[s] | 손실 0.41\n",
      "| 에폭 569 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 570 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 571 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 572 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 573 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 574 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 575 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 576 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 577 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 578 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 579 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 580 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
      "| 에폭 581 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 582 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 583 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 584 |  반복 1 / 2 | 시간 1[s] | 손실 0.41\n",
      "| 에폭 585 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 586 |  반복 1 / 2 | 시간 1[s] | 손실 0.43\n",
      "| 에폭 587 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 588 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 589 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
      "| 에폭 590 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 591 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 592 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 593 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 594 |  반복 1 / 2 | 시간 1[s] | 손실 0.39\n",
      "| 에폭 595 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 596 |  반복 1 / 2 | 시간 1[s] | 손실 0.39\n",
      "| 에폭 597 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
      "| 에폭 598 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 599 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 600 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
      "| 에폭 601 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 602 |  반복 1 / 2 | 시간 1[s] | 손실 0.33\n",
      "| 에폭 603 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 604 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
      "| 에폭 605 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
      "| 에폭 606 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 607 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 608 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 609 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 610 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 611 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
      "| 에폭 612 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 613 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 614 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 615 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 616 |  반복 1 / 2 | 시간 1[s] | 손실 0.37\n",
      "| 에폭 617 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 618 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 619 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 620 |  반복 1 / 2 | 시간 1[s] | 손실 0.40\n",
      "| 에폭 621 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 622 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 623 |  반복 1 / 2 | 시간 1[s] | 손실 0.38\n",
      "| 에폭 624 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 625 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 626 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
      "| 에폭 627 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
      "| 에폭 628 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 629 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 630 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 631 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
      "| 에폭 632 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
      "| 에폭 633 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 634 |  반복 1 / 2 | 시간 1[s] | 손실 0.37\n",
      "| 에폭 635 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 636 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
      "| 에폭 637 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 638 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
      "| 에폭 639 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 640 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 641 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
      "| 에폭 642 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
      "| 에폭 643 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 644 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
      "| 에폭 645 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 646 |  반복 1 / 2 | 시간 1[s] | 손실 0.33\n",
      "| 에폭 647 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 648 |  반복 1 / 2 | 시간 1[s] | 손실 0.38\n",
      "| 에폭 649 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 650 |  반복 1 / 2 | 시간 1[s] | 손실 0.37\n",
      "| 에폭 651 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 652 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 653 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
      "| 에폭 654 |  반복 1 / 2 | 시간 1[s] | 손실 0.41\n",
      "| 에폭 655 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 656 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 657 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 658 |  반복 1 / 2 | 시간 1[s] | 손실 0.38\n",
      "| 에폭 659 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 660 |  반복 1 / 2 | 시간 1[s] | 손실 0.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 661 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 662 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 663 |  반복 1 / 2 | 시간 1[s] | 손실 0.33\n",
      "| 에폭 664 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 665 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 666 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
      "| 에폭 667 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 668 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 669 |  반복 1 / 2 | 시간 1[s] | 손실 0.36\n",
      "| 에폭 670 |  반복 1 / 2 | 시간 1[s] | 손실 0.37\n",
      "| 에폭 671 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 672 |  반복 1 / 2 | 시간 1[s] | 손실 0.32\n",
      "| 에폭 673 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 674 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 675 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 676 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 677 |  반복 1 / 2 | 시간 1[s] | 손실 0.34\n",
      "| 에폭 678 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 679 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 680 |  반복 1 / 2 | 시간 1[s] | 손실 0.34\n",
      "| 에폭 681 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 682 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 683 |  반복 1 / 2 | 시간 1[s] | 손실 0.27\n",
      "| 에폭 684 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 685 |  반복 1 / 2 | 시간 1[s] | 손실 0.40\n",
      "| 에폭 686 |  반복 1 / 2 | 시간 1[s] | 손실 0.41\n",
      "| 에폭 687 |  반복 1 / 2 | 시간 1[s] | 손실 0.39\n",
      "| 에폭 688 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 689 |  반복 1 / 2 | 시간 1[s] | 손실 0.41\n",
      "| 에폭 690 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 691 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 692 |  반복 1 / 2 | 시간 1[s] | 손실 0.38\n",
      "| 에폭 693 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 694 |  반복 1 / 2 | 시간 1[s] | 손실 0.38\n",
      "| 에폭 695 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
      "| 에폭 696 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
      "| 에폭 697 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
      "| 에폭 698 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
      "| 에폭 699 |  반복 1 / 2 | 시간 1[s] | 손실 0.38\n",
      "| 에폭 700 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 701 |  반복 1 / 2 | 시간 1[s] | 손실 0.43\n",
      "| 에폭 702 |  반복 1 / 2 | 시간 1[s] | 손실 0.41\n",
      "| 에폭 703 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 704 |  반복 1 / 2 | 시간 1[s] | 손실 0.37\n",
      "| 에폭 705 |  반복 1 / 2 | 시간 1[s] | 손실 0.40\n",
      "| 에폭 706 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
      "| 에폭 707 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
      "| 에폭 708 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
      "| 에폭 709 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
      "| 에폭 710 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 711 |  반복 1 / 2 | 시간 1[s] | 손실 0.22\n",
      "| 에폭 712 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 713 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 714 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
      "| 에폭 715 |  반복 1 / 2 | 시간 1[s] | 손실 0.25\n",
      "| 에폭 716 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 717 |  반복 1 / 2 | 시간 1[s] | 손실 0.33\n",
      "| 에폭 718 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
      "| 에폭 719 |  반복 1 / 2 | 시간 2[s] | 손실 0.47\n",
      "| 에폭 720 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 721 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 722 |  반복 1 / 2 | 시간 2[s] | 손실 0.26\n",
      "| 에폭 723 |  반복 1 / 2 | 시간 2[s] | 손실 0.60\n",
      "| 에폭 724 |  반복 1 / 2 | 시간 2[s] | 손실 0.32\n",
      "| 에폭 725 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 726 |  반복 1 / 2 | 시간 2[s] | 손실 0.53\n",
      "| 에폭 727 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 728 |  반복 1 / 2 | 시간 2[s] | 손실 0.49\n",
      "| 에폭 729 |  반복 1 / 2 | 시간 2[s] | 손실 0.22\n",
      "| 에폭 730 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 731 |  반복 1 / 2 | 시간 2[s] | 손실 0.56\n",
      "| 에폭 732 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 733 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 734 |  반복 1 / 2 | 시간 2[s] | 손실 0.32\n",
      "| 에폭 735 |  반복 1 / 2 | 시간 2[s] | 손실 0.52\n",
      "| 에폭 736 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 737 |  반복 1 / 2 | 시간 2[s] | 손실 0.42\n",
      "| 에폭 738 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 739 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 740 |  반복 1 / 2 | 시간 2[s] | 손실 0.20\n",
      "| 에폭 741 |  반복 1 / 2 | 시간 2[s] | 손실 0.52\n",
      "| 에폭 742 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 743 |  반복 1 / 2 | 시간 2[s] | 손실 0.53\n",
      "| 에폭 744 |  반복 1 / 2 | 시간 2[s] | 손실 0.31\n",
      "| 에폭 745 |  반복 1 / 2 | 시간 2[s] | 손실 0.61\n",
      "| 에폭 746 |  반복 1 / 2 | 시간 2[s] | 손실 0.30\n",
      "| 에폭 747 |  반복 1 / 2 | 시간 2[s] | 손실 0.52\n",
      "| 에폭 748 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 749 |  반복 1 / 2 | 시간 2[s] | 손실 0.20\n",
      "| 에폭 750 |  반복 1 / 2 | 시간 2[s] | 손실 0.42\n",
      "| 에폭 751 |  반복 1 / 2 | 시간 2[s] | 손실 0.48\n",
      "| 에폭 752 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 753 |  반복 1 / 2 | 시간 2[s] | 손실 0.48\n",
      "| 에폭 754 |  반복 1 / 2 | 시간 2[s] | 손실 0.20\n",
      "| 에폭 755 |  반복 1 / 2 | 시간 2[s] | 손실 0.55\n",
      "| 에폭 756 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 757 |  반복 1 / 2 | 시간 2[s] | 손실 0.30\n",
      "| 에폭 758 |  반복 1 / 2 | 시간 2[s] | 손실 0.31\n",
      "| 에폭 759 |  반복 1 / 2 | 시간 2[s] | 손실 0.49\n",
      "| 에폭 760 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 761 |  반복 1 / 2 | 시간 2[s] | 손실 0.30\n",
      "| 에폭 762 |  반복 1 / 2 | 시간 2[s] | 손실 0.48\n",
      "| 에폭 763 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 764 |  반복 1 / 2 | 시간 2[s] | 손실 0.41\n",
      "| 에폭 765 |  반복 1 / 2 | 시간 2[s] | 손실 0.41\n",
      "| 에폭 766 |  반복 1 / 2 | 시간 2[s] | 손실 0.30\n",
      "| 에폭 767 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 768 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 769 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 770 |  반복 1 / 2 | 시간 2[s] | 손실 0.41\n",
      "| 에폭 771 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 772 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 773 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 774 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 775 |  반복 1 / 2 | 시간 2[s] | 손실 0.32\n",
      "| 에폭 776 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 777 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 778 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 779 |  반복 1 / 2 | 시간 2[s] | 손실 0.31\n",
      "| 에폭 780 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 781 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 782 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 783 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n",
      "| 에폭 784 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 785 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 786 |  반복 1 / 2 | 시간 2[s] | 손실 0.26\n",
      "| 에폭 787 |  반복 1 / 2 | 시간 2[s] | 손실 0.42\n",
      "| 에폭 788 |  반복 1 / 2 | 시간 2[s] | 손실 0.48\n",
      "| 에폭 789 |  반복 1 / 2 | 시간 2[s] | 손실 0.31\n",
      "| 에폭 790 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 791 |  반복 1 / 2 | 시간 2[s] | 손실 0.42\n",
      "| 에폭 792 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 793 |  반복 1 / 2 | 시간 2[s] | 손실 0.18\n",
      "| 에폭 794 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n",
      "| 에폭 795 |  반복 1 / 2 | 시간 2[s] | 손실 0.49\n",
      "| 에폭 796 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 797 |  반복 1 / 2 | 시간 2[s] | 손실 0.42\n",
      "| 에폭 798 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 799 |  반복 1 / 2 | 시간 2[s] | 손실 0.41\n",
      "| 에폭 800 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n",
      "| 에폭 801 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n",
      "| 에폭 802 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 803 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n",
      "| 에폭 804 |  반복 1 / 2 | 시간 2[s] | 손실 0.41\n",
      "| 에폭 805 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 806 |  반복 1 / 2 | 시간 2[s] | 손실 0.47\n",
      "| 에폭 807 |  반복 1 / 2 | 시간 2[s] | 손실 0.30\n",
      "| 에폭 808 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 809 |  반복 1 / 2 | 시간 2[s] | 손실 0.41\n",
      "| 에폭 810 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n",
      "| 에폭 811 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 812 |  반복 1 / 2 | 시간 2[s] | 손실 0.27\n",
      "| 에폭 813 |  반복 1 / 2 | 시간 2[s] | 손실 0.47\n",
      "| 에폭 814 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 815 |  반복 1 / 2 | 시간 2[s] | 손실 0.30\n",
      "| 에폭 816 |  반복 1 / 2 | 시간 2[s] | 손실 0.47\n",
      "| 에폭 817 |  반복 1 / 2 | 시간 2[s] | 손실 0.49\n",
      "| 에폭 818 |  반복 1 / 2 | 시간 2[s] | 손실 0.16\n",
      "| 에폭 819 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 820 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 821 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 822 |  반복 1 / 2 | 시간 2[s] | 손실 0.27\n",
      "| 에폭 823 |  반복 1 / 2 | 시간 2[s] | 손실 0.57\n",
      "| 에폭 824 |  반복 1 / 2 | 시간 2[s] | 손실 0.30\n",
      "| 에폭 825 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 826 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 827 |  반복 1 / 2 | 시간 2[s] | 손실 0.48\n",
      "| 에폭 828 |  반복 1 / 2 | 시간 2[s] | 손실 0.27\n",
      "| 에폭 829 |  반복 1 / 2 | 시간 2[s] | 손실 0.29\n",
      "| 에폭 830 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 831 |  반복 1 / 2 | 시간 2[s] | 손실 0.46\n",
      "| 에폭 832 |  반복 1 / 2 | 시간 2[s] | 손실 0.28\n",
      "| 에폭 833 |  반복 1 / 2 | 시간 2[s] | 손실 0.57\n",
      "| 에폭 834 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 835 |  반복 1 / 2 | 시간 2[s] | 손실 0.16\n",
      "| 에폭 836 |  반복 1 / 2 | 시간 2[s] | 손실 0.48\n",
      "| 에폭 837 |  반복 1 / 2 | 시간 2[s] | 손실 0.26\n",
      "| 에폭 838 |  반복 1 / 2 | 시간 2[s] | 손실 0.48\n",
      "| 에폭 839 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 840 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 841 |  반복 1 / 2 | 시간 2[s] | 손실 0.26\n",
      "| 에폭 842 |  반복 1 / 2 | 시간 2[s] | 손실 0.48\n",
      "| 에폭 843 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n",
      "| 에폭 844 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 845 |  반복 1 / 2 | 시간 2[s] | 손실 0.24\n",
      "| 에폭 846 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 847 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 848 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 849 |  반복 1 / 2 | 시간 2[s] | 손실 0.28\n",
      "| 에폭 850 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 851 |  반복 1 / 2 | 시간 2[s] | 손실 0.56\n",
      "| 에폭 852 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n",
      "| 에폭 853 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 854 |  반복 1 / 2 | 시간 2[s] | 손실 0.16\n",
      "| 에폭 855 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 856 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n",
      "| 에폭 857 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 858 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 859 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 860 |  반복 1 / 2 | 시간 2[s] | 손실 0.27\n",
      "| 에폭 861 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 862 |  반복 1 / 2 | 시간 2[s] | 손실 0.27\n",
      "| 에폭 863 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 864 |  반복 1 / 2 | 시간 2[s] | 손실 0.25\n",
      "| 에폭 865 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n",
      "| 에폭 866 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 867 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 868 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 869 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 870 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 871 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 872 |  반복 1 / 2 | 시간 2[s] | 손실 0.27\n",
      "| 에폭 873 |  반복 1 / 2 | 시간 2[s] | 손실 0.47\n",
      "| 에폭 874 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 875 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 876 |  반복 1 / 2 | 시간 2[s] | 손실 0.27\n",
      "| 에폭 877 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 878 |  반복 1 / 2 | 시간 2[s] | 손실 0.16\n",
      "| 에폭 879 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 880 |  반복 1 / 2 | 시간 2[s] | 손실 0.26\n",
      "| 에폭 881 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 882 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 883 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 884 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 885 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 886 |  반복 1 / 2 | 시간 2[s] | 손실 0.22\n",
      "| 에폭 887 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 888 |  반복 1 / 2 | 시간 2[s] | 손실 0.25\n",
      "| 에폭 889 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 890 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 891 |  반복 1 / 2 | 시간 2[s] | 손실 0.24\n",
      "| 에폭 892 |  반복 1 / 2 | 시간 2[s] | 손실 0.57\n",
      "| 에폭 893 |  반복 1 / 2 | 시간 2[s] | 손실 0.13\n",
      "| 에폭 894 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 895 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 896 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 897 |  반복 1 / 2 | 시간 2[s] | 손실 0.26\n",
      "| 에폭 898 |  반복 1 / 2 | 시간 2[s] | 손실 0.56\n",
      "| 에폭 899 |  반복 1 / 2 | 시간 2[s] | 손실 0.25\n",
      "| 에폭 900 |  반복 1 / 2 | 시간 2[s] | 손실 0.33\n",
      "| 에폭 901 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 902 |  반복 1 / 2 | 시간 2[s] | 손실 0.24\n",
      "| 에폭 903 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 904 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 905 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 906 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 907 |  반복 1 / 2 | 시간 2[s] | 손실 0.33\n",
      "| 에폭 908 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 909 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 910 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 911 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 912 |  반복 1 / 2 | 시간 2[s] | 손실 0.33\n",
      "| 에폭 913 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 914 |  반복 1 / 2 | 시간 2[s] | 손실 0.13\n",
      "| 에폭 915 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 916 |  반복 1 / 2 | 시간 2[s] | 손실 0.55\n",
      "| 에폭 917 |  반복 1 / 2 | 시간 2[s] | 손실 0.25\n",
      "| 에폭 918 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 919 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 920 |  반복 1 / 2 | 시간 2[s] | 손실 0.25\n",
      "| 에폭 921 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 922 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 923 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 924 |  반복 1 / 2 | 시간 2[s] | 손실 0.22\n",
      "| 에폭 925 |  반복 1 / 2 | 시간 2[s] | 손실 0.25\n",
      "| 에폭 926 |  반복 1 / 2 | 시간 2[s] | 손실 0.32\n",
      "| 에폭 927 |  반복 1 / 2 | 시간 2[s] | 손실 0.47\n",
      "| 에폭 928 |  반복 1 / 2 | 시간 2[s] | 손실 0.33\n",
      "| 에폭 929 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 930 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 931 |  반복 1 / 2 | 시간 2[s] | 손실 0.23\n",
      "| 에폭 932 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 933 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 934 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 935 |  반복 1 / 2 | 시간 2[s] | 손실 0.13\n",
      "| 에폭 936 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 937 |  반복 1 / 2 | 시간 2[s] | 손실 0.24\n",
      "| 에폭 938 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 939 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 940 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 941 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 942 |  반복 1 / 2 | 시간 2[s] | 손실 0.23\n",
      "| 에폭 943 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 944 |  반복 1 / 2 | 시간 2[s] | 손실 0.22\n",
      "| 에폭 945 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 946 |  반복 1 / 2 | 시간 2[s] | 손실 0.32\n",
      "| 에폭 947 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 948 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 949 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 950 |  반복 1 / 2 | 시간 2[s] | 손실 0.23\n",
      "| 에폭 951 |  반복 1 / 2 | 시간 2[s] | 손실 0.33\n",
      "| 에폭 952 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 953 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 954 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 955 |  반복 1 / 2 | 시간 2[s] | 손실 0.22\n",
      "| 에폭 956 |  반복 1 / 2 | 시간 2[s] | 손실 0.32\n",
      "| 에폭 957 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 958 |  반복 1 / 2 | 시간 2[s] | 손실 0.22\n",
      "| 에폭 959 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 960 |  반복 1 / 2 | 시간 2[s] | 손실 0.33\n",
      "| 에폭 961 |  반복 1 / 2 | 시간 2[s] | 손실 0.31\n",
      "| 에폭 962 |  반복 1 / 2 | 시간 2[s] | 손실 0.24\n",
      "| 에폭 963 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 964 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 965 |  반복 1 / 2 | 시간 2[s] | 손실 0.22\n",
      "| 에폭 966 |  반복 1 / 2 | 시간 2[s] | 손실 0.33\n",
      "| 에폭 967 |  반복 1 / 2 | 시간 2[s] | 손실 0.31\n",
      "| 에폭 968 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 969 |  반복 1 / 2 | 시간 2[s] | 손실 0.33\n",
      "| 에폭 970 |  반복 1 / 2 | 시간 2[s] | 손실 0.22\n",
      "| 에폭 971 |  반복 1 / 2 | 시간 2[s] | 손실 0.32\n",
      "| 에폭 972 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 973 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 974 |  반복 1 / 2 | 시간 2[s] | 손실 0.23\n",
      "| 에폭 975 |  반복 1 / 2 | 시간 2[s] | 손실 0.20\n",
      "| 에폭 976 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 977 |  반복 1 / 2 | 시간 2[s] | 손실 0.23\n",
      "| 에폭 978 |  반복 1 / 2 | 시간 2[s] | 손실 0.33\n",
      "| 에폭 979 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 980 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 981 |  반복 1 / 2 | 시간 2[s] | 손실 0.20\n",
      "| 에폭 982 |  반복 1 / 2 | 시간 2[s] | 손실 0.33\n",
      "| 에폭 983 |  반복 1 / 2 | 시간 2[s] | 손실 0.23\n",
      "| 에폭 984 |  반복 1 / 2 | 시간 2[s] | 손실 0.53\n",
      "| 에폭 985 |  반복 1 / 2 | 시간 2[s] | 손실 0.10\n",
      "| 에폭 986 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 987 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 988 |  반복 1 / 2 | 시간 2[s] | 손실 0.42\n",
      "| 에폭 989 |  반복 1 / 2 | 시간 2[s] | 손실 0.32\n",
      "| 에폭 990 |  반복 1 / 2 | 시간 2[s] | 손실 0.23\n",
      "| 에폭 991 |  반복 1 / 2 | 시간 2[s] | 손실 0.31\n",
      "| 에폭 992 |  반복 1 / 2 | 시간 2[s] | 손실 0.32\n",
      "| 에폭 993 |  반복 1 / 2 | 시간 2[s] | 손실 0.22\n",
      "| 에폭 994 |  반복 1 / 2 | 시간 2[s] | 손실 0.42\n",
      "| 에폭 995 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 996 |  반복 1 / 2 | 시간 2[s] | 손실 0.23\n",
      "| 에폭 997 |  반복 1 / 2 | 시간 2[s] | 손실 0.32\n",
      "| 에폭 998 |  반복 1 / 2 | 시간 2[s] | 손실 0.31\n",
      "| 에폭 999 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 1000 |  반복 1 / 2 | 시간 2[s] | 손실 0.31\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you [ 1.091  0.927  0.901 -1.646  0.908]\n",
      "say [ 1.192 -1.18  -1.216 -0.326 -1.214]\n",
      "goodbye [0.785 1.06  1.065 0.571 1.07 ]\n",
      "and [ 0.276 -0.945 -1.016 -1.781 -0.984]\n",
      "i [0.787 1.041 1.054 0.584 1.057]\n",
      "hello [ 1.1    0.947  0.916 -1.646  0.923]\n",
      ". [ 1.461 -1.063 -1.065  1.626 -1.11 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')  \n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from ch03.simple_cbow import SimpleCBOW\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "\n",
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "word_vecs = model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec 보충\n",
    "\n",
    "### CBOW 모델과 확률\n",
    "\n",
    "word2vec의 CBOW 모델은 수학적으로 다음과 같이 쓸 수 있다.\n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 3-22.png\" width=\"50%\" height=\"20%\" /> \n",
    "\n",
    "이때 맥락으로 $W_{t-1}$와 $W_{t+1}$이 주어졌을 때 타깃이 $W_t$가 될 확률은 수식으로 다음과 같다.\n",
    "\n",
    "$$P(W_t | W_{t-1},W_{t+1})$$\n",
    "\n",
    "기존의 교차 엔트로피 오차식으로 부터 다음과 같은 식을 유도할 수 있다.\n",
    "\n",
    "$$ L = -\\log P(W_t | W_{t-1},W_{t+1})$$\n",
    "\n",
    "이를 __음의 로그 가능도__라 부르며 말뭉치 전체로 확장하면 다음과 같이 된다. 이를 CBOW 모델의 손실함수로 사용하자.\n",
    "\n",
    "$$ L = -{1 \\over T}\\sum_{t=1}^{T}\\log P(W_t | W_{t-1},W_{t+1})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip-gram 모델\n",
    "\n",
    "skip-gram모델은 CBOW에서 다루는 맥락과 타깃을 역전시킨 모델이다.\n",
    "\n",
    "다음 그림을 보면 단번에 이해가 될 것이다.\n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 3-23.png\" width=\"50%\" height=\"20%\" /> \n",
    "\n",
    "<img src=\"deep_learning_2_images/fig 3-24.png\" width=\"50%\" height=\"20%\" /> \n",
    "\n",
    "하나의 입력에 대해 맥락의 수만큼 출력이 존재하고 각 출력에 대한 손실함수의 합을 최소화하는 방향으로 학습이 이루어진다.\n",
    "\n",
    "확률은 CBOW와 반대로 다음과 같다.\n",
    "\n",
    "$$P(W_{t-1},W_{t+1} | W_t)$$\n",
    "\n",
    "각각의 단어가 조건부 독립이라는 가정 하에 다음과 같다.\n",
    "\n",
    "$$P(W_{t-1},W_{t+1} | W_t) = P(W_{t-1}| W_t)P(W_{t+1}| W_t)$$ \n",
    "\n",
    "corpus 전체에 대한 skip-gram모델의 손실함수는 다음과 같다.\n",
    "\n",
    "$$\\begin{align*}L &=  -P(W_{t-1},W_{t+1} | W_t) \\\\ &=  P(W_{t-1}| W_t)P(W_{t+1}| W_t) \\\\ &=  -(\\log P(W_{t-1}| W_t)+\\log P(W_{t+1}| W_t))\\end{align*}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 통계 기반 vs 추론 기반\n",
    "\n",
    "두 기법에는 큰 차이가 있다. \n",
    "\n",
    "통계 기반 기법은 corpus의 전체 통계로부터 1회 학습하는 것이고 추론 기반 기법은 corpus의 일부분씩 여러 번보면서 학습하는 것이다.\n",
    "\n",
    "만약 새로운 단어가 갱신되었을 때 통계 기반은 처음부터 다시 해야 되는데 추론 기반은 가중치만 다시 학습해주면 되므로 이런 점에서 추론 기반이 더 우세하다.\n",
    "\n",
    "통계 기반은 주로 단어의 유사성이 인코딩되며 추론 기반은 한층 복잡한 단어 사이의 유추 문제도 풀 수 있다.\n",
    "\n",
    "흔히 추론 기반이 통계 기반보다 좋다고 오해하지만 의외로 우열을 가릴 수 없다고 하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이번 장에서 배운 내용\n",
    "\n",
    "    * 추론 기반 기법은 추측하는 것이 목적이며, 그 부산물로 단어으이 분산 표현을 얻을 수 있다.\n",
    "    \n",
    "    * word2vec은 추론 기반 기법이며, 단순한 2층 신경망이다.\n",
    "    \n",
    "    * word2vec은 skip-gram 모델과 CBOW 모델을 제공한다.\n",
    "    \n",
    "    * CBOW 모델은 여러 단어(맥락)으로부터 하나의 단어(타깃)을 추축한다.\n",
    "    \n",
    "    * 반대로 skip-gram 모델은 하나의 단어(타깃)로부터 다수의 단어(맥락)를 추측한다.\n",
    "    \n",
    "    * word2vec은 가중치를 다시 학습할 수 있으므로, 단어의 분산 표현 갱신이안 새로운 단어 추가를 효율적으로 수행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec 속도 개선\n",
    "\n",
    "앞에서 word2vec의 구조를 배우고 CBOW모델을 구현했다. 구현에는 몇 가지 문제가 있다. \n",
    "\n",
    "가장 큰 문제는 corpus에 포함된 어휘 수가 많아지면 계산량도 커진다는 점이다.\n",
    "\n",
    "이번 장에서 두 가지 방법으로 속도 개선을 할 것이다.\n",
    "\n",
    "첫 번째, Embedding이라는 새로운 계층을 도입한다.\n",
    "\n",
    "두 번째, negative sampling이라는 새로운 손실 함수를 도입한다.\n",
    "\n",
    "## word2vec 개선 (1)\n",
    "\n",
    "어휘 수가 많아지면 다음의 두 계산에서 병목 현상이 일어난다.\n",
    "\n",
    "1. 입력층의 원핫 표현과 가중치 행렬 $\\textbf{W}_in$의 곱 계산 -- Embedding으로 해결\n",
    "    \n",
    "2. 은닉층과 가중치 행렬 $\\textbf{W}_{out}$의 곱 및 Softmax 계층의 계산 -- Negative sampling으로 해결\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding 계층\n",
    "\n",
    "원핫벡터와 가중치 행렬을 곱하는 것이므로 결과적으로 수행하는 일은 단지 행렬의 특정 행을 추출하는 것뿐이다.\n",
    "\n",
    "따라서 원핫표현으로의 변환과 MatMul 계층의 행렬 곱 계산은 사실 필요가 없다.\n",
    "\n",
    "그래서 가중치 매개변수로부터 '단어 ID에 해당하는 행'을 추출하는 계층을 만들면 된다. 그 계층을 Embedding 계층이라 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding 계층 구현\n",
    "\n",
    "그냥 원하는 행 인덱스로 불러오면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  4  5]\n",
      " [ 0  1  2]\n",
      " [ 9 10 11]\n",
      " [ 0  1  2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "W = np.arange(21).reshape(7,3)\n",
    "idx = np.array([1, 0, 3, 0])\n",
    "\n",
    "print (W[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n여기에서는 가중치 W와 크기가 같은 행렬 dW를 만들고, dW의 특정 행에 기울기를 할당했다.\\n\\n그러나 최종적으로 하고 시은 일은 가중치 W를 갱신하는 것이므로 일부러 dW와 같은 (W와 같은 크기의) 행렬을 만들 필요가 없다.\\n\\n갱신하려는 행 번호(idx)와 그 기울기(dout)를 따라 저장해두면, 이 정보로부터 가중치(W)의 특정 행만 갱신할 수 있다.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zros_like(W)]\n",
    "        self.idx = None\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dW = self.grads\n",
    "        dW[...] = 0\n",
    "        dW[self.idx] = dout # 나쁜 예\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "'''\n",
    "여기에서는 가중치 W와 크기가 같은 행렬 dW를 만들고, dW의 특정 행에 기울기를 할당했다.\n",
    "\n",
    "그러나 최종적으로 하고 시은 일은 가중치 W를 갱신하는 것이므로 일부러 dW와 같은 (W와 같은 크기의) 행렬을 만들 필요가 없다.\n",
    "\n",
    "갱신하려는 행 번호(idx)와 그 기울기(dout)를 따라 저장해두면, 이 정보로부터 가중치(W)의 특정 행만 갱신할 수 있다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 문제가 하나 있다. idx의 원소가 중복될 때 발생하는 문제이다. \n",
    "\n",
    "위의 그림과 같이 dh의 각 행 갓을 idx가 가리키는 장소에 할당해보자. 그러면 dW의 0번째 행에 2개의 값이 할당된다. 먼저 쓰여진 값이 사라진다.\n",
    "\n",
    "이 중복 문제를 해결하기 위해 '할당'이 아닌 '더하기'를 해야 한다.\n",
    "\n",
    "즉, dh의 각 행의 값을 dW의 해당 행에 더해주자. \n",
    "\n",
    "__<질문>__\n",
    "\n",
    "왜 더해야 하는 걸까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, dout):\n",
    "    dW = self.grads\n",
    "    dw[...] = 0\n",
    "    \n",
    "    for i, word_id in enumerate(self.idx):\n",
    "        dW[word_id] += dout[i]\n",
    "        # 혹은\n",
    "        # np.add.at(dW, self.idx, dout)\n",
    "        # for문 보다 넘파이 내장함수를 쓰는 것이 훨씬 더 빠르다.\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec 개선 (2)\n",
    "\n",
    "이번에는 __Negative sampling__을 이용하여 은닉층의 뉴런과 가중치 행렬의 곱 그리고 Softmax 계층의 계산에서 발생하는 병목을 해결해보자!!\n",
    "\n",
    "입력층$\\cdot$은닉층 뉴런이 100만개 은닉층의 뉴런이 100개인 경우를 생각해보자.\n",
    "\n",
    "앞 절에서 Embedding을 통해 입력층 계산을 향상시켰다. 계산지체를 일으키는 남은 부분이 2가지이다.\n",
    "\n",
    "1. 은닉층의 뉴런과 가중치 행렬($\\textbf{W}_{out}$)의 곱\n",
    "\n",
    "2. Softmax 계층의 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다중 분류에서 이진 분류로\n",
    "\n",
    "이 기법의 핵심은 '다중 분류'를 '이진 분류'로 근사하는 것이다.\n",
    "\n",
    "아래의 사진과 같이 출력층에는 뉴런을 하나만 준비하면 된다.\n",
    "\n",
    "<img src=\"./deep_learning_2_images/negative2.png\"  width=\"50%\" height=\"50%\">\n",
    "\n",
    "위에서 보듯이 출력층의 뉴런은 하나뿐이다. 따라서 은닉층과 출력 측의 가중치 행렬의 내적은 'say'에 해당하는 열만을 추출하고, 그 추출된 벡터와 \n",
    "\n",
    "은닉층 뉴런과의 내적을 계산하면 끝이다.\n",
    "\n",
    "이전까지의 출력층에서는 모든 단어를 대상으로 계산을 수행했지만, 여기에서는 하나의 단어에 주목하여 그 점수만을 게산하는 게 차이이다. \n",
    "\n",
    "\n",
    "그리고 시그모이드 함수를 이용하여 확률로 변환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시그모이드 함수와 교차 엔트로피 오차\n",
    "\n",
    "시그모이드 함수를 이용해 확률을 구하고 손실 함수로 '교차 엔트로피 오차'를 사용한다.\n",
    "\n",
    "$$ L = -(t\\log y + (1-t)log(1-y)) $$\n",
    "\n",
    "y는 시그모이드 함수의 출력, t는 정답 레이블(1 or 0)\n",
    "\n",
    "<img src=\"./deep_learning_2_images/negative4.png\"  width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다중 분류에서 이진 분류로\n",
    "\n",
    "아래의 사진은 다중 분류를 수행하는 CBOW 모델이다.\n",
    "\n",
    "<img src=\"./deep_learning_2_images/negative5.png\"  width=\"50%\" height=\"50%\">\n",
    "\n",
    "아래의 사진은 이진 분류를 수행하는 word2vec 모델이다.\n",
    "\n",
    "<img src=\"./deep_learning_2_images/negative7.png\"  width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "        \n",
    "\n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W * h, aixs=1)\n",
    "        \n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0],1)\n",
    "        \n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네거티브 샘플링\n",
    "\n",
    "지금까지 배운 것으로 '다중 분류'에서 '이진 분류'로 변환할 수 있다. 그러나 이것만으로는 문제가 다 해결되지 않는다.\n",
    "\n",
    "그러나 현재 긍정적인 예(정답)에 대해서만 학습했기 떄문에 오답을 입력하면 어떤 결과가 나올지 확실하지 않다.\n",
    "\n",
    "우리가 정말 해야 하는 것은 정답에 대해서는 sigmoid 계층의 출력을 1에 가깝게 만들고, 오답에 대해서는 sigmoid 계층의 출력을 0에 가깝게 만드는 것이다. \n",
    "\n",
    "아래의 사진과 같다.\n",
    "\n",
    "<img src=\"./deep_learning_2_images/negative10.png\"  width=\"50%\" height=\"50%\">\n",
    "\n",
    "모든 부정적 예를 대상으로 하여 이진 분류를 학습시키는 것은 불가능하다. 그래서 근사적인 해법으로, 적은 수의 부정적 예를 샘플링해 학습시킨다.\n",
    "\n",
    "이것이 바로 'Negative sampling'기법이다. \n",
    "\n",
    "정리하면, negative sampling 기법은 긍정적 예를 타깃으로 한 경우의 손실을 구한다. 그와 동시에 부정적 예를 몇 개 샘플링하여 그에 대해서도 손실을 구한다. 그리고 각각의 데이터의 손실을 더한 값을 최종 손실로 한다.\n",
    "\n",
    "아래의 사진이 negative sampling의 계산 그래프이다.\n",
    "\n",
    "<img src=\"./deep_learning_2_images/negative11.png\"  width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### negative sampling의 sampling 기법\n",
    "\n",
    "corpus에서 자주 등장하는 단어를 많이 추출하고 드물게 등장하는 단어를 적게 추출하는 것이다. \n",
    "\n",
    "1. corpus에서 각 단어의 출현 횟수를 구해 확률분포로 나타낸다.\n",
    "\n",
    "\n",
    "2. 구한 확률분포에 따라서 샘플링을 수행한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      ".\n",
      "['you' '.' '.' 'hello' 'goodbye']\n",
      "['.' 'say' 'you' 'hello' 'I']\n",
      "you\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 0에서 9까지의 숫자 중 하나를 무작위로 샘플링\n",
    "print (np.random.choice(10))\n",
    "\n",
    "# words에서 하나만 무작위로 샘플링\n",
    "words = ['you', 'say', 'goodbye', 'I', 'hello', '.']\n",
    "print (np.random.choice(words))\n",
    "\n",
    "# 5개만 무작위로 샘플링(중복okay)\n",
    "print (np.random.choice(words, size=5))\n",
    "\n",
    "# 5개만 무작위로 샘플링(중복ㄴㄴ)\n",
    "print (np.random.choice(words, size=5, replace=False))\n",
    "\n",
    "# 확률분포에 따라 샘플링\n",
    "p = [.5, .1, .05, .2, .05, .1]\n",
    "print (np.random.choice(words, p=p))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 가지 수정해야 하는 부분이 있다.\n",
    "\n",
    "$$P'(w_i) = {P(w_i)^{0.75} \\over \\sum_{j}^{n}P(w_j)^{0.75}}$$\n",
    "\n",
    "위와 같이 확률 값에 1미만의 값을 제곱해줘야 한다. 그래야 원래 작았던 확률 값이 살짝 커져서 출현 확률이 낮은 단어도 버리지 않게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64196878 0.33150408 0.02652714]\n"
     ]
    }
   ],
   "source": [
    "p = [.7, .29, .01]\n",
    "new_p = np.power(p, .75)\n",
    "new_p /= np.sum(new_p)\n",
    "print (new_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UnigramSampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-86ae16738513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnigramSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnegative_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_negative_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'UnigramSampler' is not defined"
     ]
    }
   ],
   "source": [
    "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n",
    "power = 0.75\n",
    "sample_size = 2\n",
    "\n",
    "sampler = UnigramSampler(corpus, power, sample_size)\n",
    "target = np.array([1, 3, 0])\n",
    "negative_sample = sampler.get_negative_sample(target) \n",
    "print (negative_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-33-282c7c279a14>, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-282c7c279a14>\"\u001b[0;36m, line \u001b[0;32m34\u001b[0m\n\u001b[0;31m    dscore = 10.backward(dout)\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power=.75, sample_size=5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
    "        self.params, self.grads = [], []\n",
    "        \n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negtive_sample = self.sample.get_negative_sample(target)\n",
    "        \n",
    "        # 긍정적 예 순전파\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "        \n",
    "        # 부정적 예 순전파\n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
    "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for 10, 11 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = 10.backward(dout)\n",
    "            dh += 11.backward(dscore)\n",
    "            \n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개선판 word2vec 학습\n",
    "\n",
    "### CBOW 모델 구현\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import Embedding\n",
    "from ch04.negative_sampling_layer import NegativeSamplingLoss\n",
    "\n",
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.in_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = Embedding(W_in) # embedding 계층 사용\n",
    "            self.in_layers.append(layer)\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=.75, sample_size=5)\n",
    "        \n",
    "        # 모든 가중치와 기울기를 배열에 모은다.\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layers.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "        \n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:, i])\n",
    "        h *= 1 / len(self.in_layers)    \n",
    "        loss = self.ns_loss.forward(h, target)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1 / len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        return None   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CBOW 모델 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 9295 | 시간 0[s] | 손실 4.16\n",
      "| 에폭 1 |  반복 21 / 9295 | 시간 5[s] | 손실 4.16\n",
      "| 에폭 1 |  반복 41 / 9295 | 시간 10[s] | 손실 4.15\n",
      "| 에폭 1 |  반복 61 / 9295 | 시간 14[s] | 손실 4.12\n",
      "| 에폭 1 |  반복 81 / 9295 | 시간 19[s] | 손실 4.05\n",
      "| 에폭 1 |  반복 101 / 9295 | 시간 24[s] | 손실 3.93\n",
      "| 에폭 1 |  반복 121 / 9295 | 시간 28[s] | 손실 3.78\n",
      "| 에폭 1 |  반복 141 / 9295 | 시간 33[s] | 손실 3.63\n",
      "| 에폭 1 |  반복 161 / 9295 | 시간 40[s] | 손실 3.49\n",
      "| 에폭 1 |  반복 181 / 9295 | 시간 45[s] | 손실 3.37\n",
      "| 에폭 1 |  반복 201 / 9295 | 시간 50[s] | 손실 3.25\n",
      "| 에폭 1 |  반복 221 / 9295 | 시간 55[s] | 손실 3.16\n",
      "| 에폭 1 |  반복 241 / 9295 | 시간 60[s] | 손실 3.09\n",
      "| 에폭 1 |  반복 261 / 9295 | 시간 65[s] | 손실 3.00\n",
      "| 에폭 1 |  반복 281 / 9295 | 시간 69[s] | 손실 2.95\n",
      "| 에폭 1 |  반복 301 / 9295 | 시간 75[s] | 손실 2.92\n",
      "| 에폭 1 |  반복 321 / 9295 | 시간 80[s] | 손실 2.87\n",
      "| 에폭 1 |  반복 341 / 9295 | 시간 85[s] | 손실 2.84\n",
      "| 에폭 1 |  반복 361 / 9295 | 시간 90[s] | 손실 2.81\n",
      "| 에폭 1 |  반복 381 / 9295 | 시간 96[s] | 손실 2.77\n",
      "| 에폭 1 |  반복 401 / 9295 | 시간 101[s] | 손실 2.78\n",
      "| 에폭 1 |  반복 421 / 9295 | 시간 106[s] | 손실 2.74\n",
      "| 에폭 1 |  반복 441 / 9295 | 시간 112[s] | 손실 2.74\n",
      "| 에폭 1 |  반복 461 / 9295 | 시간 116[s] | 손실 2.72\n",
      "| 에폭 1 |  반복 481 / 9295 | 시간 122[s] | 손실 2.72\n",
      "| 에폭 1 |  반복 501 / 9295 | 시간 127[s] | 손실 2.70\n",
      "| 에폭 1 |  반복 521 / 9295 | 시간 132[s] | 손실 2.69\n",
      "| 에폭 1 |  반복 541 / 9295 | 시간 137[s] | 손실 2.65\n",
      "| 에폭 1 |  반복 561 / 9295 | 시간 141[s] | 손실 2.66\n",
      "| 에폭 1 |  반복 581 / 9295 | 시간 147[s] | 손실 2.65\n",
      "| 에폭 1 |  반복 601 / 9295 | 시간 152[s] | 손실 2.64\n",
      "| 에폭 1 |  반복 621 / 9295 | 시간 157[s] | 손실 2.64\n",
      "| 에폭 1 |  반복 641 / 9295 | 시간 161[s] | 손실 2.63\n",
      "| 에폭 1 |  반복 661 / 9295 | 시간 166[s] | 손실 2.63\n",
      "| 에폭 1 |  반복 681 / 9295 | 시간 171[s] | 손실 2.61\n",
      "| 에폭 1 |  반복 701 / 9295 | 시간 176[s] | 손실 2.62\n",
      "| 에폭 1 |  반복 721 / 9295 | 시간 181[s] | 손실 2.61\n",
      "| 에폭 1 |  반복 741 / 9295 | 시간 186[s] | 손실 2.58\n",
      "| 에폭 1 |  반복 761 / 9295 | 시간 191[s] | 손실 2.60\n",
      "| 에폭 1 |  반복 781 / 9295 | 시간 196[s] | 손실 2.61\n",
      "| 에폭 1 |  반복 801 / 9295 | 시간 201[s] | 손실 2.56\n",
      "| 에폭 1 |  반복 821 / 9295 | 시간 206[s] | 손실 2.61\n",
      "| 에폭 1 |  반복 841 / 9295 | 시간 211[s] | 손실 2.60\n",
      "| 에폭 1 |  반복 861 / 9295 | 시간 216[s] | 손실 2.60\n",
      "| 에폭 1 |  반복 881 / 9295 | 시간 221[s] | 손실 2.60\n",
      "| 에폭 1 |  반복 901 / 9295 | 시간 226[s] | 손실 2.57\n",
      "| 에폭 1 |  반복 921 / 9295 | 시간 232[s] | 손실 2.58\n",
      "| 에폭 1 |  반복 941 / 9295 | 시간 237[s] | 손실 2.57\n",
      "| 에폭 1 |  반복 961 / 9295 | 시간 242[s] | 손실 2.57\n",
      "| 에폭 1 |  반복 981 / 9295 | 시간 247[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1001 / 9295 | 시간 252[s] | 손실 2.56\n",
      "| 에폭 1 |  반복 1021 / 9295 | 시간 257[s] | 손실 2.56\n",
      "| 에폭 1 |  반복 1041 / 9295 | 시간 262[s] | 손실 2.57\n",
      "| 에폭 1 |  반복 1061 / 9295 | 시간 267[s] | 손실 2.55\n",
      "| 에폭 1 |  반복 1081 / 9295 | 시간 272[s] | 손실 2.55\n",
      "| 에폭 1 |  반복 1101 / 9295 | 시간 276[s] | 손실 2.56\n",
      "| 에폭 1 |  반복 1121 / 9295 | 시간 281[s] | 손실 2.56\n",
      "| 에폭 1 |  반복 1141 / 9295 | 시간 286[s] | 손실 2.53\n",
      "| 에폭 1 |  반복 1161 / 9295 | 시간 291[s] | 손실 2.57\n",
      "| 에폭 1 |  반복 1181 / 9295 | 시간 296[s] | 손실 2.56\n",
      "| 에폭 1 |  반복 1201 / 9295 | 시간 301[s] | 손실 2.54\n",
      "| 에폭 1 |  반복 1221 / 9295 | 시간 306[s] | 손실 2.54\n",
      "| 에폭 1 |  반복 1241 / 9295 | 시간 311[s] | 손실 2.53\n",
      "| 에폭 1 |  반복 1261 / 9295 | 시간 316[s] | 손실 2.55\n",
      "| 에폭 1 |  반복 1281 / 9295 | 시간 321[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1301 / 9295 | 시간 327[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1321 / 9295 | 시간 332[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 1341 / 9295 | 시간 337[s] | 손실 2.53\n",
      "| 에폭 1 |  반복 1361 / 9295 | 시간 342[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1381 / 9295 | 시간 347[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 1401 / 9295 | 시간 352[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 1421 / 9295 | 시간 358[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1441 / 9295 | 시간 363[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1461 / 9295 | 시간 368[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 1481 / 9295 | 시간 373[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 1501 / 9295 | 시간 378[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1521 / 9295 | 시간 383[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 1541 / 9295 | 시간 388[s] | 손실 2.54\n",
      "| 에폭 1 |  반복 1561 / 9295 | 시간 394[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1581 / 9295 | 시간 399[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1601 / 9295 | 시간 404[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1621 / 9295 | 시간 410[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 1641 / 9295 | 시간 415[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1661 / 9295 | 시간 420[s] | 손실 2.49\n",
      "| 에폭 1 |  반복 1681 / 9295 | 시간 424[s] | 손실 2.49\n",
      "| 에폭 1 |  반복 1701 / 9295 | 시간 429[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 1721 / 9295 | 시간 434[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 1741 / 9295 | 시간 439[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 1761 / 9295 | 시간 444[s] | 손실 2.49\n",
      "| 에폭 1 |  반복 1781 / 9295 | 시간 449[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1801 / 9295 | 시간 453[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 1821 / 9295 | 시간 458[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 1841 / 9295 | 시간 463[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1861 / 9295 | 시간 468[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 1881 / 9295 | 시간 473[s] | 손실 2.49\n",
      "| 에폭 1 |  반복 1901 / 9295 | 시간 477[s] | 손실 2.49\n",
      "| 에폭 1 |  반복 1921 / 9295 | 시간 482[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 1941 / 9295 | 시간 487[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 1961 / 9295 | 시간 491[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 1981 / 9295 | 시간 496[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2001 / 9295 | 시간 501[s] | 손실 2.49\n",
      "| 에폭 1 |  반복 2021 / 9295 | 시간 506[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 2041 / 9295 | 시간 510[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 2061 / 9295 | 시간 515[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 2081 / 9295 | 시간 520[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2101 / 9295 | 시간 524[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2121 / 9295 | 시간 529[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2141 / 9295 | 시간 534[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2161 / 9295 | 시간 539[s] | 손실 2.49\n",
      "| 에폭 1 |  반복 2181 / 9295 | 시간 543[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 2201 / 9295 | 시간 548[s] | 손실 2.49\n",
      "| 에폭 1 |  반복 2221 / 9295 | 시간 553[s] | 손실 2.49\n",
      "| 에폭 1 |  반복 2241 / 9295 | 시간 557[s] | 손실 2.49\n",
      "| 에폭 1 |  반복 2261 / 9295 | 시간 562[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2281 / 9295 | 시간 567[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 2301 / 9295 | 시간 572[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 2321 / 9295 | 시간 577[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 2341 / 9295 | 시간 581[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 2361 / 9295 | 시간 586[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2381 / 9295 | 시간 591[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 2401 / 9295 | 시간 596[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2421 / 9295 | 시간 601[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2441 / 9295 | 시간 606[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 2461 / 9295 | 시간 610[s] | 손실 2.49\n",
      "| 에폭 1 |  반복 2481 / 9295 | 시간 615[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2501 / 9295 | 시간 620[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2521 / 9295 | 시간 624[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2541 / 9295 | 시간 630[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2561 / 9295 | 시간 636[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2581 / 9295 | 시간 641[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2601 / 9295 | 시간 646[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 2621 / 9295 | 시간 650[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2641 / 9295 | 시간 655[s] | 손실 2.49\n",
      "| 에폭 1 |  반복 2661 / 9295 | 시간 660[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2681 / 9295 | 시간 664[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 2701 / 9295 | 시간 669[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 2721 / 9295 | 시간 674[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 2741 / 9295 | 시간 678[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2761 / 9295 | 시간 683[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 2781 / 9295 | 시간 688[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 2801 / 9295 | 시간 692[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2821 / 9295 | 시간 697[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2841 / 9295 | 시간 702[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 2861 / 9295 | 시간 706[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2881 / 9295 | 시간 712[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 2901 / 9295 | 시간 717[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2921 / 9295 | 시간 723[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 2941 / 9295 | 시간 728[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 2961 / 9295 | 시간 734[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 2981 / 9295 | 시간 739[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 3001 / 9295 | 시간 746[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3021 / 9295 | 시간 751[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 3041 / 9295 | 시간 756[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 3061 / 9295 | 시간 760[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 3081 / 9295 | 시간 765[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 3101 / 9295 | 시간 770[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 3121 / 9295 | 시간 774[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 3141 / 9295 | 시간 779[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 3161 / 9295 | 시간 783[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3181 / 9295 | 시간 789[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 3201 / 9295 | 시간 794[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 3221 / 9295 | 시간 799[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 3241 / 9295 | 시간 803[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 3261 / 9295 | 시간 808[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 3281 / 9295 | 시간 813[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3301 / 9295 | 시간 818[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 3321 / 9295 | 시간 823[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 3341 / 9295 | 시간 828[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 3361 / 9295 | 시간 833[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 3381 / 9295 | 시간 838[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3401 / 9295 | 시간 843[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 3421 / 9295 | 시간 848[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 3441 / 9295 | 시간 852[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 3461 / 9295 | 시간 857[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 3481 / 9295 | 시간 862[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 3501 / 9295 | 시간 867[s] | 손실 2.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 3521 / 9295 | 시간 872[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 3541 / 9295 | 시간 877[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3561 / 9295 | 시간 882[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 3581 / 9295 | 시간 887[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 3601 / 9295 | 시간 892[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 3621 / 9295 | 시간 897[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 3641 / 9295 | 시간 901[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 3661 / 9295 | 시간 906[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 3681 / 9295 | 시간 911[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 3701 / 9295 | 시간 916[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 3721 / 9295 | 시간 921[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 3741 / 9295 | 시간 926[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 3761 / 9295 | 시간 931[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 3781 / 9295 | 시간 935[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 3801 / 9295 | 시간 940[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 3821 / 9295 | 시간 945[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 3841 / 9295 | 시간 950[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 3861 / 9295 | 시간 955[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 3881 / 9295 | 시간 960[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 3901 / 9295 | 시간 965[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 3921 / 9295 | 시간 970[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 3941 / 9295 | 시간 974[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3961 / 9295 | 시간 979[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3981 / 9295 | 시간 984[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 4001 / 9295 | 시간 989[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 4021 / 9295 | 시간 993[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 4041 / 9295 | 시간 998[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 4061 / 9295 | 시간 1003[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 4081 / 9295 | 시간 1008[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 4101 / 9295 | 시간 1013[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 4121 / 9295 | 시간 1017[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 4141 / 9295 | 시간 1022[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 4161 / 9295 | 시간 1027[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 4181 / 9295 | 시간 1031[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 4201 / 9295 | 시간 1036[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 4221 / 9295 | 시간 1041[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 4241 / 9295 | 시간 1045[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 4261 / 9295 | 시간 1050[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 4281 / 9295 | 시간 1054[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 4301 / 9295 | 시간 1059[s] | 손실 2.38\n",
      "| 에폭 1 |  반복 4321 / 9295 | 시간 1063[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 4341 / 9295 | 시간 1068[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 4361 / 9295 | 시간 1073[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 4381 / 9295 | 시간 1078[s] | 손실 2.38\n",
      "| 에폭 1 |  반복 4401 / 9295 | 시간 1082[s] | 손실 2.38\n",
      "| 에폭 1 |  반복 4421 / 9295 | 시간 1087[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 4441 / 9295 | 시간 1092[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 4461 / 9295 | 시간 1096[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 4481 / 9295 | 시간 1101[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 4501 / 9295 | 시간 1105[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 4521 / 9295 | 시간 1110[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 4541 / 9295 | 시간 1115[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 4561 / 9295 | 시간 1119[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 4581 / 9295 | 시간 1124[s] | 손실 2.38\n",
      "| 에폭 1 |  반복 4601 / 9295 | 시간 1128[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 4621 / 9295 | 시간 1133[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 4641 / 9295 | 시간 1138[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 4661 / 9295 | 시간 1143[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 4681 / 9295 | 시간 1148[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 4701 / 9295 | 시간 1153[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 4721 / 9295 | 시간 1159[s] | 손실 2.38\n",
      "| 에폭 1 |  반복 4741 / 9295 | 시간 1164[s] | 손실 2.38\n",
      "| 에폭 1 |  반복 4761 / 9295 | 시간 1169[s] | 손실 2.38\n",
      "| 에폭 1 |  반복 4781 / 9295 | 시간 1173[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 4801 / 9295 | 시간 1178[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 4821 / 9295 | 시간 1182[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 4841 / 9295 | 시간 1187[s] | 손실 2.38\n",
      "| 에폭 1 |  반복 4861 / 9295 | 시간 1192[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 4881 / 9295 | 시간 1197[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 4901 / 9295 | 시간 1202[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 4921 / 9295 | 시간 1206[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 4941 / 9295 | 시간 1211[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 4961 / 9295 | 시간 1216[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 4981 / 9295 | 시간 1221[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 5001 / 9295 | 시간 1226[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 5021 / 9295 | 시간 1231[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 5041 / 9295 | 시간 1236[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 5061 / 9295 | 시간 1240[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 5081 / 9295 | 시간 1245[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 5101 / 9295 | 시간 1250[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 5121 / 9295 | 시간 1255[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 5141 / 9295 | 시간 1260[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 5161 / 9295 | 시간 1265[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 5181 / 9295 | 시간 1269[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 5201 / 9295 | 시간 1274[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 5221 / 9295 | 시간 1279[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 5241 / 9295 | 시간 1284[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 5261 / 9295 | 시간 1289[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 5281 / 9295 | 시간 1294[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 5301 / 9295 | 시간 1299[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 5321 / 9295 | 시간 1303[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 5341 / 9295 | 시간 1309[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 5361 / 9295 | 시간 1314[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 5381 / 9295 | 시간 1320[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 5401 / 9295 | 시간 1325[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 5421 / 9295 | 시간 1331[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 5441 / 9295 | 시간 1335[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 5461 / 9295 | 시간 1340[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 5481 / 9295 | 시간 1345[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 5501 / 9295 | 시간 1350[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 5521 / 9295 | 시간 1355[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 5541 / 9295 | 시간 1359[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 5561 / 9295 | 시간 1364[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 5581 / 9295 | 시간 1369[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 5601 / 9295 | 시간 1374[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 5621 / 9295 | 시간 1379[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 5641 / 9295 | 시간 1383[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 5661 / 9295 | 시간 1388[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 5681 / 9295 | 시간 1393[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 5701 / 9295 | 시간 1398[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 5721 / 9295 | 시간 1403[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 5741 / 9295 | 시간 1408[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 5761 / 9295 | 시간 1412[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 5781 / 9295 | 시간 1417[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 5801 / 9295 | 시간 1422[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 5821 / 9295 | 시간 1427[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 5841 / 9295 | 시간 1432[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 5861 / 9295 | 시간 1437[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 5881 / 9295 | 시간 1442[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 5901 / 9295 | 시간 1446[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 5921 / 9295 | 시간 1451[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 5941 / 9295 | 시간 1456[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 5961 / 9295 | 시간 1461[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 5981 / 9295 | 시간 1466[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6001 / 9295 | 시간 1471[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6021 / 9295 | 시간 1476[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6041 / 9295 | 시간 1481[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 6061 / 9295 | 시간 1486[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6081 / 9295 | 시간 1490[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 6101 / 9295 | 시간 1495[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6121 / 9295 | 시간 1500[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 6141 / 9295 | 시간 1505[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6161 / 9295 | 시간 1510[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 6181 / 9295 | 시간 1514[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 6201 / 9295 | 시간 1519[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6221 / 9295 | 시간 1524[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 6241 / 9295 | 시간 1529[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 6261 / 9295 | 시간 1534[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6281 / 9295 | 시간 1538[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6301 / 9295 | 시간 1543[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 6321 / 9295 | 시간 1548[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6341 / 9295 | 시간 1553[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 6361 / 9295 | 시간 1558[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6381 / 9295 | 시간 1563[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 6401 / 9295 | 시간 1568[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6421 / 9295 | 시간 1573[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6441 / 9295 | 시간 1577[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6461 / 9295 | 시간 1582[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 6481 / 9295 | 시간 1587[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 6501 / 9295 | 시간 1592[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6521 / 9295 | 시간 1597[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6541 / 9295 | 시간 1602[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6561 / 9295 | 시간 1606[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6581 / 9295 | 시간 1611[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 6601 / 9295 | 시간 1616[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6621 / 9295 | 시간 1621[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 6641 / 9295 | 시간 1626[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 6661 / 9295 | 시간 1632[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 6681 / 9295 | 시간 1637[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 6701 / 9295 | 시간 1641[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6721 / 9295 | 시간 1646[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 6741 / 9295 | 시간 1651[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 6761 / 9295 | 시간 1656[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 6781 / 9295 | 시간 1661[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6801 / 9295 | 시간 1665[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 6821 / 9295 | 시간 1671[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 6841 / 9295 | 시간 1675[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 6861 / 9295 | 시간 1680[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 6881 / 9295 | 시간 1685[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6901 / 9295 | 시간 1690[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 6921 / 9295 | 시간 1694[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6941 / 9295 | 시간 1699[s] | 손실 2.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 6961 / 9295 | 시간 1704[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6981 / 9295 | 시간 1709[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 7001 / 9295 | 시간 1713[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 7021 / 9295 | 시간 1718[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7041 / 9295 | 시간 1723[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 7061 / 9295 | 시간 1728[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 7081 / 9295 | 시간 1733[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 7101 / 9295 | 시간 1737[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 7121 / 9295 | 시간 1742[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 7141 / 9295 | 시간 1747[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 7161 / 9295 | 시간 1752[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 7181 / 9295 | 시간 1756[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 7201 / 9295 | 시간 1761[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 7221 / 9295 | 시간 1766[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 7241 / 9295 | 시간 1771[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7261 / 9295 | 시간 1776[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7281 / 9295 | 시간 1780[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 7301 / 9295 | 시간 1785[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7321 / 9295 | 시간 1790[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7341 / 9295 | 시간 1795[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 7361 / 9295 | 시간 1800[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 7381 / 9295 | 시간 1805[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7401 / 9295 | 시간 1809[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 7421 / 9295 | 시간 1814[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 7441 / 9295 | 시간 1819[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 7461 / 9295 | 시간 1824[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 7481 / 9295 | 시간 1829[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 7501 / 9295 | 시간 1833[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 7521 / 9295 | 시간 1838[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 7541 / 9295 | 시간 1843[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 7561 / 9295 | 시간 1848[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 7581 / 9295 | 시간 1853[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 7601 / 9295 | 시간 1857[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7621 / 9295 | 시간 1862[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 7641 / 9295 | 시간 1867[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7661 / 9295 | 시간 1872[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 7681 / 9295 | 시간 1876[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 7701 / 9295 | 시간 1881[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 7721 / 9295 | 시간 1886[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 7741 / 9295 | 시간 1891[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 7761 / 9295 | 시간 1896[s] | 손실 2.22\n",
      "| 에폭 1 |  반복 7781 / 9295 | 시간 1900[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 7801 / 9295 | 시간 1906[s] | 손실 2.22\n",
      "| 에폭 1 |  반복 7821 / 9295 | 시간 1911[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 7841 / 9295 | 시간 1916[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 7861 / 9295 | 시간 1921[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 7881 / 9295 | 시간 1926[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 7901 / 9295 | 시간 1930[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 7921 / 9295 | 시간 1935[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 7941 / 9295 | 시간 1940[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7961 / 9295 | 시간 1945[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 7981 / 9295 | 시간 1949[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 8001 / 9295 | 시간 1954[s] | 손실 2.22\n",
      "| 에폭 1 |  반복 8021 / 9295 | 시간 1959[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8041 / 9295 | 시간 1964[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 8061 / 9295 | 시간 1969[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8081 / 9295 | 시간 1973[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 8101 / 9295 | 시간 1978[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 8121 / 9295 | 시간 1983[s] | 손실 2.22\n",
      "| 에폭 1 |  반복 8141 / 9295 | 시간 1988[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8161 / 9295 | 시간 1992[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 8181 / 9295 | 시간 1997[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 8201 / 9295 | 시간 2002[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 8221 / 9295 | 시간 2007[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 8241 / 9295 | 시간 2011[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 8261 / 9295 | 시간 2016[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 8281 / 9295 | 시간 2021[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 8301 / 9295 | 시간 2026[s] | 손실 2.20\n",
      "| 에폭 1 |  반복 8321 / 9295 | 시간 2031[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 8341 / 9295 | 시간 2036[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 8361 / 9295 | 시간 2041[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 8381 / 9295 | 시간 2045[s] | 손실 2.22\n",
      "| 에폭 1 |  반복 8401 / 9295 | 시간 2050[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8421 / 9295 | 시간 2055[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 8441 / 9295 | 시간 2060[s] | 손실 2.22\n",
      "| 에폭 1 |  반복 8461 / 9295 | 시간 2065[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 8481 / 9295 | 시간 2069[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8501 / 9295 | 시간 2074[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8521 / 9295 | 시간 2079[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 8541 / 9295 | 시간 2084[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 8561 / 9295 | 시간 2089[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 8581 / 9295 | 시간 2093[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 8601 / 9295 | 시간 2098[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8621 / 9295 | 시간 2103[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 8641 / 9295 | 시간 2108[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8661 / 9295 | 시간 2113[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 8681 / 9295 | 시간 2117[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 8701 / 9295 | 시간 2122[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 8721 / 9295 | 시간 2127[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 8741 / 9295 | 시간 2132[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 8761 / 9295 | 시간 2137[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 8781 / 9295 | 시간 2142[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 8801 / 9295 | 시간 2147[s] | 손실 2.22\n",
      "| 에폭 1 |  반복 8821 / 9295 | 시간 2152[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 8841 / 9295 | 시간 2156[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 8861 / 9295 | 시간 2161[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8881 / 9295 | 시간 2166[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 8901 / 9295 | 시간 2170[s] | 손실 2.22\n",
      "| 에폭 1 |  반복 8921 / 9295 | 시간 2175[s] | 손실 2.22\n",
      "| 에폭 1 |  반복 8941 / 9295 | 시간 2180[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 8961 / 9295 | 시간 2184[s] | 손실 2.20\n",
      "| 에폭 1 |  반복 8981 / 9295 | 시간 2189[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 9001 / 9295 | 시간 2193[s] | 손실 2.20\n",
      "| 에폭 1 |  반복 9021 / 9295 | 시간 2198[s] | 손실 2.20\n",
      "| 에폭 1 |  반복 9041 / 9295 | 시간 2203[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 9061 / 9295 | 시간 2207[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 9081 / 9295 | 시간 2212[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 9101 / 9295 | 시간 2217[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 9121 / 9295 | 시간 2221[s] | 손실 2.22\n",
      "| 에폭 1 |  반복 9141 / 9295 | 시간 2226[s] | 손실 2.20\n",
      "| 에폭 1 |  반복 9161 / 9295 | 시간 2231[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 9181 / 9295 | 시간 2235[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 9201 / 9295 | 시간 2240[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 9221 / 9295 | 시간 2244[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 9241 / 9295 | 시간 2249[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 9261 / 9295 | 시간 2254[s] | 손실 2.20\n",
      "| 에폭 1 |  반복 9281 / 9295 | 시간 2258[s] | 손실 2.22\n",
      "| 에폭 2 |  반복 1 / 9295 | 시간 2262[s] | 손실 2.22\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common import config\n",
    "import pickle\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from ch04.cbow import CBOW\n",
    "from common.util import create_contexts_target, to_cpu, to_gpu\n",
    "from dataset import ptb\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "\n",
    "# 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "if config.GPU:\n",
    "    contexts, target = t+gpu(contexts), to_gpu(target)\n",
    "    \n",
    "# 모델 등 생성\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "# 학습 시작\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "# 나중에 사용할 수 있도록 필요한 데이터 저장\n",
    "word_vecs = model.word_vecs\n",
    "if config.GPU:\n",
    "    words_vecs = to_cpu(word_vecs)\n",
    "params = {}\n",
    "params['word_vecs'] = word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = 'cbow_params.pkl'\n",
    "with open(pkl_file, 'wb') as f:\n",
    "    pickle.dump(params, f, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW 모델 평가\n",
    "\n",
    "단어 몇개에 대해 거리가 가장 가까운 단어들을 뽑아보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cbow_params.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2ac9fabcf678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# pkl_file = 'skipgram_params.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkl_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mword_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word_vecs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cbow_params.pkl'"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import most_similar, analogy\n",
    "import pickle\n",
    "\n",
    "\n",
    "pkl_file = 'cbow_params.pkl'\n",
    "# pkl_file = 'skipgram_params.pkl'\n",
    "\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    word_vecs = params['word_vecs']\n",
    "    word_to_id = params['word_to_id']\n",
    "    id_to_word = params['id_to_word']\n",
    "\n",
    "# 가장 비슷한(most similar) 단어 뽑기\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)\n",
    "\n",
    "# 유추(analogy) 작업\n",
    "print('-'*50)\n",
    "analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이번 장에서 배운 내용\n",
    "\n",
    "* Embedding 계층은 단어의 분산 표현을 담고 있으며, 순전파 시 지정한 단어 ID의 벡터를 추출한다.\n",
    "    \n",
    "* word2vec의 어휘 수의 증가엥 비례하여 계산량도 증가하므로, 근사치로 계산하는 빠른 기법을 사용하려면 좋다.\n",
    "        \n",
    "* negative sampling은 부정적 예를 몇 개 샘플링하는 기법으로, 이를 이용하면 다중 분류를 이진 분류처럼 취급할 수 있다.\n",
    "        \n",
    "* word2vec으로 얻은 단어의 분산 표현에는 단어의 의미가 녹아들어 있으며, 비슷한 맥락에서 사용되는 단어는 단어 벡터 공간에서 가까이           위치한다.\n",
    "        \n",
    "* word2vec의 단어의 분산 표현을 이용하면 유추 문제를 벡터의 덧셈과 뺄셈으로 풀 수 있게 된다.\n",
    "        \n",
    "* word2vec은 전이학습측면에서 특히 중요하며, 그 단엉의 분산 표현은 다양한 자연어 처리 작업에 이용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
